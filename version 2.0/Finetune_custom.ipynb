{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac72cd29-d098-432a-9e7c-14f787f981e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - /home/aalla4/SML\n"
     ]
    }
   ],
   "source": [
    "# setting up the directory where we want to store the models\n",
    "import os\n",
    "\n",
    "print(\"Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - \" + os.getcwd())\n",
    "os.environ['HF_HOME'] = \"/home/aalla4\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/home/aalla4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38daf9f-e838-4656-84a2-755195ed1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 08:46:29.207519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of logical cores = 48\n",
      "CUDA available:  True\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import T5Config\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# getting the CPU and GPU count\n",
    "\n",
    "print(\"Total number of logical cores = \" + str(os.cpu_count()))  # This shows logical cores not the physical cores\n",
    "LOGICAL_CORES = os.cpu_count()\n",
    "USABLE_CPU_CORES = LOGICAL_CORES - 1    # YOU CAN CHANGE THIS ACCORDING TO THE CPU AVAILABILITIES\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d415bd-c660-4ec2-bea8-f534244558e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer \n",
    "\n",
    "# model_name = \"t5-base\" # ---------------------------------------------------------------------------------------------\n",
    "model_name = \"t5-large\" \n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f11fb4-20de-40fd-ace5-8c4246586a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation of special tokens used:\n",
      "- <extra_id_99> is used as a placeholder for [CONTEXT]\n",
      "- <extra_id_98> is used as a placeholder for [DIFFICULTY]\n",
      "- <extra_id_97> is used as a placeholder for [TAG]\n"
     ]
    }
   ],
   "source": [
    "# Making some special tokens as placeholders/seperators for the input \n",
    "\n",
    "# we don't waste any additional tokens in this process to seperate the inputs into sections\n",
    "special_tokens_description = {\n",
    "    \"<extra_id_99>\": \"[CONTEXT]\",      # Represents the context section of the input\n",
    "    \"<extra_id_98>\": \"[DIFFICULTY]\",   # Represents the difficulty section of the input - 'easy'/ 'medium'/ 'hard'\n",
    "    \"<extra_id_97>\": \"[TAG]\"           # Represents the tag section of the input\n",
    "}\n",
    "\n",
    "print(\"Explanation of special tokens used:\")\n",
    "for token, description in special_tokens_description.items():\n",
    "    print(f\"- {token} is used as a placeholder for {description}\")\n",
    "\n",
    "# if we can simply use the existing tokens we don't wanna increase additional special tokens\n",
    "# special_tokens = {'additional_special_tokens': ['[CONTEXT]', '[DIFFICULTY]', '[TAG]']}\n",
    "\n",
    "# Get current additional special tokens from the tokenizer\n",
    "# existing_special_tokens = tokenizer.special_tokens_map.get('additional_special_tokens', [])\n",
    "\n",
    "# tokenizer.add_special_tokens(special_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8b284-89cf-4d83-b96d-d301e2a89e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a6ca3-18c9-4ab4-aa08-e33fc2d49fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338367b4-03e4-4645-8d24-d88a83264756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501381e-89ba-434b-be98-7f897d8035cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps for 5 datasets combined in certain proportions\n",
    "\n",
    "# only run these below cells if you want to train your model on the 5 mixed datasets following a certain proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da56c0c-c096-4efe-a4bf-bdcf930ef070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "# datasets used are squad v1, hotpotqa, openbookqa, drop, boolq\n",
    "# the datasets returns a DatasetDict with \"train\" and \"validation\" splits\n",
    "\n",
    "print(\"Loading the datasets ........\")\n",
    "\n",
    "# Load HotpotQA (distractor)\n",
    "hotpotqa = load_dataset(\"hotpot_qa\", \"distractor\",  trust_remote_code=True)\n",
    "\n",
    "# SQUAD (V1)\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# OpenBookQA (additional)\n",
    "openbookqa = load_dataset(\"openbookqa\", \"additional\")\n",
    "\n",
    "# Boolq dataset\n",
    "boolq = load_dataset(\"boolq\")\n",
    "\n",
    "# Drop dataset\n",
    "drop = load_dataset(\"drop\")\n",
    "\n",
    "print(\"Completed loading the datasets ........\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd4e54-017e-4aca-80d4-c55c54867816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions of the datasets\n",
    "\n",
    "# Preprocessing, combining and shuffling datasets only uses LOGICAL_CORES/ CPU's and not done on GPU's\n",
    "print(\"Starting Preprocesing; Available CPU Cores to use - \"+ str(USABLE_CPU_CORES))\n",
    "\n",
    "def preprocess_hotpotqa(example):  \n",
    "    \"hotpotq - used for short answer question generation, with focus on multi-hop sentences rater than forming the question with just a single line in the whole context\"      \n",
    "\n",
    "    answer = example['answer']\n",
    "    if len(answer.split()) < 12:                    \n",
    "        tag = \"very short answer\"\n",
    "    else:\n",
    "        tag = \"short answer\"\n",
    "    difficulty = example['level']                   # Extract difficulty - 'easy'/ 'medium'/ 'hard'\n",
    "\n",
    "    # thought of just keeping the supporting sentences instead of the all the sentences in the supporting titles, but the context length is too short for this approach           \n",
    "    supporting_facts = example[\"supporting_facts\"]\n",
    "    supporting_titles = set([t for t in supporting_facts['title']])          # Use set to avoid duplicates\n",
    "    context_titles = example['context']['title']\n",
    "    context_sentences = example['context']['sentences']\n",
    "\n",
    "    supporting_sentences = []\n",
    "\n",
    "    for idx, title in enumerate(context_titles):\n",
    "        if title in supporting_titles:\n",
    "            # Add all sentences under this title\n",
    "            req_sentences = []\n",
    "            for sent in context_sentences[idx]:\n",
    "                req_sentences.append(f\"{sent}\")\n",
    "            sentence_block = \" \".join(req_sentences)  \n",
    "            supporting_sentences.append(f\"{title}, {sentence_block}\")\n",
    "\n",
    "    short_context = \" \".join(supporting_sentences)\n",
    "    \n",
    "    # Randomly decide whether to include the answer or not as not always the answer will be likely to be provided by the instructor\n",
    "    include_answer = random.choices([True, False], weights=[15, 85], k=1)[0]\n",
    "\n",
    "    if include_answer:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>[{answer}] {short_context}\"     # Prepare the model input\n",
    "    else:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{short_context}\"\n",
    "       \n",
    "    target_text = f\"{example['question']}\"                                                                       # Prepare the target output\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text) \n",
    "    \n",
    "    # Tokenize both input and output\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Attach labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_squad(example):\n",
    "    \"squad - used for short answer question generation, with multiple questions generated with the same context\"\n",
    "    \n",
    "    answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "    if len(answer.split()) < 12:\n",
    "        tag = \"very short answer\"\n",
    "    else:\n",
    "        tag = \"short answer\"\n",
    "    difficulty = \"easy\"\n",
    "    context = example['context']\n",
    "\n",
    "    include_answer = random.choices([True, False], weights=[15, 85], k=1)[0]\n",
    "    if include_answer:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>[{answer}] {context}\"\n",
    "    else:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "\n",
    "    target_text = example[\"question\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_openbookqa(example):\n",
    "    \"openbookqa - used for multiple choice question generation\"\n",
    "   \n",
    "    # difficulty is calculated based on the human score and clarity\n",
    "    clarity = example.get(\"clarity\", 0)\n",
    "    human_score = example.get(\"human_score\", 1)\n",
    "    \n",
    "    # Filter out low clarity examples\n",
    "    # if clarity <= 1:\n",
    "    #     return None       # discard low-clarity / low-quality examples\n",
    "        # return {}\n",
    "\n",
    "    # Assign difficulty\n",
    "    if clarity > 1.8 and human_score < 1:\n",
    "        difficulty = \"hard\" \n",
    "    else:                             # [ 1 < clarity <= 1.8 ] and if [ clarity > 1.8 and human_score > 1 ]\n",
    "        difficulty = \"medium\"\n",
    "\n",
    "    tag = \"multiple choice question\"\n",
    "    \n",
    "    # Build the context using fact1 (from additional)\n",
    "    fact1 = example.get(\"fact1\", \"\")                # example.get(\"fact1\", \"\") is safe — it gives you a default value if the key is missing, example[\"fact1\"] will raise a KeyError if 'fact1' is missing.\n",
    "    \n",
    "    # Construct the multiple choice question format with answer marked\n",
    "    question_stem = example[\"question_stem\"]\n",
    "    answer_key = example[\"answerKey\"]  # 'A', 'B', 'C', 'D'\n",
    "    choices = example[\"choices\"]       # dict with \"text\" and \"label\"\n",
    "    \n",
    "    # Extract choice texts\n",
    "    choices_text = choices[\"text\"]\n",
    "    \n",
    "    # Get index of correct answer\n",
    "    idx = ord(answer_key) - ord('A')\n",
    "    # Correct answer text\n",
    "    answer_text = choices_text[idx]\n",
    "    \n",
    "    # Remove correct answer\n",
    "    other_options = [opt for i, opt in enumerate(choices_text) if i != idx]\n",
    "    choices_str = \"; \".join(other_options)\n",
    "\n",
    "    \n",
    "    target_text = f\"{question_stem}; answer:[{answer_text}]; wrong options:({choices_str})\"\n",
    "\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{fact1}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"clarity\"] = clarity                 # Add clarity for later filtering\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_boolq(example):\n",
    "    \"boolq - used for true/ false question generation\"\n",
    "\n",
    "    # difficulty tagging is not available in BoolQ, so we default it\n",
    "    difficulty = \"medium\"\n",
    "    tag = \"true or false question\"\n",
    "\n",
    "    # Extract question and answer\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]          # this is a bool value: True / False\n",
    "    passage = example[\"passage\"]\n",
    "\n",
    "    target_text = f\"{question}; answer:[{'true' if answer else 'false'}]\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{passage}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_drop(example):\n",
    "    \"drop - used for very short answer question generation, with multiple questions generated with the same context, it  also includes some math calculations that has to be done by the model\"\n",
    "\n",
    "    tag = \"one word answer\"\n",
    "    difficulty = \"medium\"\n",
    "    question = example.get(\"question\", \"\")\n",
    "    passage = example.get(\"passage\", \"\")\n",
    "    \n",
    "    # Retrieve the first available answer from 'answers_spans' (string or list of strings)\n",
    "    answers = example.get(\"answers_spans\", {}).get(\"spans\", [])\n",
    "    if not answers:\n",
    "        return None                                           # Skip if no answer available\n",
    "\n",
    "    answer = answers[0]      \n",
    "    \n",
    "    target_text = f\"{question}; answer:[{answer}]\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{passage}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c38d15-c7d5-4fc0-9326-5d28ee168585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into train and validation & Preprocessing the datasets individually\n",
    "\n",
    "# Preprocess both splits separately for both datasets\n",
    "# put the .select(range(5)) steps for testing out the input and output results and printing them ******\n",
    "# hotpotqa_train = hotpotqa[\"train\"].select(range(5)).map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES)\n",
    "# hotpotqa_train = hotpotqa[\"train\"].select(range(5)).map(preprocess_hotpotqa)\n",
    "\n",
    "# trail\n",
    "# drop_train = drop[\"train\"].select(range(5)).map(preprocess_drop)\n",
    "# drop_val = drop[\"validation\"].select(range(5)).map(preprocess_drop)\n",
    "# boolq_train = boolq[\"train\"].select(range(5)).map(preprocess_boolq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6d03c-589f-4b8c-a605-7c6f9cd5e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_train = hotpotqa[\"train\"].map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=hotpotqa[\"train\"].column_names)\n",
    "hotpotqa_val = hotpotqa[\"validation\"].map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=hotpotqa[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21b008-7844-4b9f-bd85-61052794bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = squad[\"train\"].map(preprocess_squad, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=squad[\"train\"].column_names)\n",
    "squad_val = squad[\"validation\"].map(preprocess_squad, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=squad[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80778a-0d74-4536-85b2-8cf3fa1851ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "openbookqa_train = openbookqa[\"train\"].map(preprocess_openbookqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=openbookqa[\"train\"].column_names).filter( lambda x: (x.get(\"clarity\", 0) > 1)).remove_columns([\"clarity\"])\n",
    "openbookqa_val = openbookqa[\"validation\"].map(preprocess_openbookqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=openbookqa[\"validation\"].column_names).filter( lambda x: (x.get(\"clarity\", 0) > 1)).remove_columns([\"clarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b246a-f95a-43c0-984e-f44fac97bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_train = boolq[\"train\"].map(preprocess_boolq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=boolq[\"train\"].column_names)\n",
    "boolq_val = boolq[\"validation\"].map(preprocess_boolq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=boolq[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eb109-5e0d-4f68-9188-68a9e7be0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = drop[\"train\"].map(preprocess_drop, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=drop[\"train\"].column_names)\n",
    "drop_val = drop[\"validation\"].map(preprocess_drop, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=drop[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6f148-cd3c-40db-936b-25f56a11f0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb6780-fe28-438f-85da-1299f11c5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportionally sample per dataset every batch\n",
    "\n",
    "datasets_dict = {\n",
    "    \"hotpot\": hotpotqa_train,\n",
    "    \"squad\": squad_train,\n",
    "    \"openbook\": openbookqa_train,\n",
    "    \"boolq\": boolq_train,\n",
    "    \"drop\": drop_train,\n",
    "}\n",
    "\n",
    "proportions = {\n",
    "    \"hotpot\": 0.25,\n",
    "    \"squad\": 0.15,\n",
    "    \"openbook\": 0.30,\n",
    "    \"boolq\": 0.25,\n",
    "    \"drop\": 0.05\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84d867-b6ee-435e-9d01-ee9389939679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class ProportionalBatchSampler(Sampler):\n",
    "    def __init__(self, datasets_dict, proportions, batch_size):\n",
    "        self.datasets_dict = datasets_dict\n",
    "        self.proportions = proportions\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Pre-calc: how many samples per dataset per batch\n",
    "        self.samples_per_dataset = {\n",
    "            k: max(1, int(batch_size * p))\n",
    "            for k, p in proportions.items()\n",
    "        }\n",
    "\n",
    "        # Make sure total == batch_size\n",
    "        diff = batch_size - sum(self.samples_per_dataset.values())\n",
    "        if diff > 0:\n",
    "            largest_key = max(self.samples_per_dataset, key=lambda x: self.samples_per_dataset[x])\n",
    "            self.samples_per_dataset[largest_key] += diff\n",
    "\n",
    "        # Convert dataset to list of indices\n",
    "        self.index_pools = {\n",
    "            k: list(range(len(ds)))\n",
    "            for k, ds in datasets_dict.items()\n",
    "        }\n",
    "\n",
    "        # Shuffle each dataset's index pool\n",
    "        for k in self.index_pools:\n",
    "            random.shuffle(self.index_pools[k])\n",
    "\n",
    "        # Total batches = smallest number of batches any dataset can support\n",
    "        self.total_batches = min([\n",
    "            len(v) // self.samples_per_dataset[k]\n",
    "            for k, v in self.index_pools.items()\n",
    "        ])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.total_batches):\n",
    "            batch_indices = []\n",
    "            for k in self.index_pools:\n",
    "                take = self.samples_per_dataset[k]\n",
    "                batch_indices.extend(self.index_pools[k][:take])\n",
    "                del self.index_pools[k][:take]\n",
    "            random.shuffle(batch_indices)\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3929ea-23c9-4c67-9a09-b1dd01473cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ProportionalDataLoader(DataLoader):\n",
    "    def __init__(self, datasets_dict, proportions, batch_size):\n",
    "        self.datasets_dict = datasets_dict\n",
    "        merged_dataset = concatenate_datasets(list(datasets_dict.values()))\n",
    "        sampler = ProportionalBatchSampler(datasets_dict, proportions, batch_size)\n",
    "        super().__init__(\n",
    "            merged_dataset,\n",
    "            batch_sampler=sampler,\n",
    "            collate_fn=trainer.data_collator\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0e59a-5d1d-49d8-aeae-4c88afc180a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61862db1-bc6d-4314-b83e-51c30378f421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d179945-1a69-4a38-924c-0682ff6c378d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5376e06-2eb5-4699-86f2-48ad5a185b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3909c27-c36d-4bbe-aa6c-2d6127cdbe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max positional embeddings supported by model - t5-large:  512\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer \n",
    "\n",
    "# model_name = \"t5-base\"                            # -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "model_name = \"t5-large\" \n",
    "\n",
    "save_path = \"./T5large_Question_Generation\"           # ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "config = T5Config.from_pretrained(\n",
    "    model_name,\n",
    "    dropout_rate=0.1,           # encoder/decoder FFN dropout\n",
    "    attention_dropout_rate=0.1, # self-attention dropout\n",
    ")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config = config)\n",
    "\n",
    "print(f\"Max positional embeddings supported by model - {model_name}: \", model.config.n_positions)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394937d-f6e2-4d8b-99ef-a14bd8614d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d3d5d-074f-4775-9aa7-edc9a6af6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the dataset and Loading the Tokenized Dataset if Preprocessing is already done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97dc1947-0b6a-44a2-9485-4b0aca9e62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset paths - choosing the dataset\n",
    "\n",
    "\n",
    "#  ----------------------------------------------------------------------------------------------\n",
    "# Choose Training set\n",
    "\n",
    "# custom dataset \n",
    "tokenized_trainset_path = \"./dataset/tokenized_custom_trainset.parquet\"\n",
    "\n",
    "# 5 datasets mixed\n",
    "# tokenized_trainset_path = \"./dataset/tokenized_trainset.parquet\"\n",
    "\n",
    "# 5 datasets mixed proportionally\n",
    "# tokenized_trainset_path = \"using 5 datasets mixed proportionally\"\n",
    "\n",
    "# Load from Parquet ---------- uncomment this\n",
    "tokenized_train_dataset = Dataset.from_parquet(tokenized_trainset_path)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# Validation set\n",
    "\n",
    "\n",
    "# you only use the val set when needed but it can always be loaded up from the already preprocessed parquet file\n",
    "# tokenized_valset_path = \"./dataset/tokenized_valset.parquet\"\n",
    "# tokenized_val_dataset = Dataset.from_parquet(tokenized_valset_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0f3e9-fc49-4c6c-aaa3-49d0aa254d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63825cc7-cbc8-4eec-ab36-a6bb2b9efa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using the dataset =  ./dataset/tokenized_custom_trainset.parquet\n",
      " Using the below parameters for training:\n",
      " epochs = 30 \n",
      " batch size = 8 \n",
      " learning rate = 1e-05 \n",
      " warmup steps = 10 \n",
      " weight decay = 0.001\n",
      " logging steps = 10\n"
     ]
    }
   ],
   "source": [
    "# prarmeters to change when you want to test out and experiment with various values depending on the dataset which you are using\n",
    "\n",
    "num_of_epochs = 30\n",
    "batch_size = 8\n",
    "\n",
    "learning_rate = 1e-5\n",
    "warmup_steps = 10\n",
    "weight_decay = 1e-3\n",
    "\n",
    "# for the model saving strategy\n",
    "save_steps = 500\n",
    "\n",
    "logging_steps = 10\n",
    "\n",
    "print(f\" Using the dataset =  {tokenized_trainset_path}\\n Using the below parameters for training:\\n epochs = {num_of_epochs} \\n batch size = {batch_size} \\n learning rate = {learning_rate} \\n warmup steps = {warmup_steps} \\n weight decay = {weight_decay}\\n logging steps = {logging_steps}\")\n",
    "\n",
    "# print(proportions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51947aad-c9c1-456b-8065-8aac7b88cfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5b134cb-9696-4920-a6bc-887478a490e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the current device:  cuda\n",
      "Device updated to: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Defning the Training args \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"This is the current device: \", device)\n",
    "print(\"Device updated to:\", model.device)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,              # directory where the training logs, checkpoints, and evaluation results (like metrics) are saved during the training process   \n",
    "    \n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_of_epochs,                                     \n",
    "    weight_decay=1e-3,                          \n",
    "\n",
    "    # generally Batch size = per_device_train_batch_size * per_device_train_batch_size\n",
    "    # -----------------------------------------------------------------------\n",
    "    # per_device_train_batch_size=batch_size,                    # ****** if using GPU's = 16; if using CPU's = 1 or 2\n",
    "    # # gradient_accumulation_steps=1,                     # Increase this if you need to simulate larger batch sizes, without running into 'Out or Memory' errors when memory is limited\n",
    "    # per_device_eval_batch_size=batch_size,\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "\n",
    "    # this is not useful for CPU based training as hugging face trainer handles multi-core utilization automatically based on the system configuration\n",
    "    dataloader_num_workers= USABLE_CPU_CORES,          # ****** for optimal use of CPU and not wasting GPU time [ this helps in loading the next batch of data into the VRAM ]\n",
    "\n",
    "    # Print validation loss every epoch\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # eval_strategy=\"epoch\",                                                                                                   # need to change this for every new experiment\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Print and logs the training loss of the training data\n",
    "    logging_strategy=\"steps\",   \n",
    "    logging_steps=logging_steps,                                   # ****** if using GPU = 100; if using CPU = 1 or 2 \n",
    "\n",
    "    # saves model at the end of every epoch\n",
    "    \n",
    "    # save_strategy=\"epoch\",  \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    \n",
    "    save_total_limit=1,\n",
    "    # save_total_limit=2,\n",
    "\n",
    "    # report_to=\"none\",  # Disable default logging\n",
    "    \n",
    "    logging_dir= save_path + \"/logs\",           # save logs to a directory\n",
    "    report_to=\"tensorboard\",                    # Reports to TensorBoard\n",
    "    log_level='info',                           # Set logging level to 'info' to see the logs in the terminal\n",
    "    # run this command in your terminal ~ tensorboard --logdir=./output_dir/runs\n",
    "    # and open 'http://localhost:6006/' to monitor the logs [loss over the training]\n",
    "\n",
    "    fp16=False                                   # ***** Mixed precision for faster training on A100; this won't work on CPU's\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4565a90c-fc40-48c1-b48d-457fc62d25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging in the Terminal\n",
    "class LogCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step {state.global_step}: {logs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076fbef-e719-41c7-a4af-fca399fbaae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143121a2-9d87-4484-bee1-37a768c6ae76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cba19c-9baa-4108-b63b-5b184e214c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e8f2f06-bd12-4a7c-b57f-1d113cae3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# for training on the custom dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=tokenized_train_dataset,            # your test set\n",
    "    # eval_dataset=tokenized_val_dataset,               # your validation set\n",
    "\n",
    "    # train_dataset=tokenized_train_dataset.select(range(100)),    \n",
    "    # eval_dataset=tokenized_val_dataset.select(range(100)),\n",
    "    \n",
    "    # data_collator=data_collator, ----------------------------------------------------------------\n",
    "    \n",
    "    callbacks=[LogCallback()]                   # *** to print the logs in the terminal\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# for training on the 5 different datasets mixed together and loaded from the tokenized parquet files\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "    \n",
    "#     train_dataset=tokenized_train_dataset,            # your test set\n",
    "#     eval_dataset=tokenized_val_dataset,               # your validation set\n",
    "\n",
    "#     # train_dataset=tokenized_train_dataset.select(range(100)),    \n",
    "#     # eval_dataset=tokenized_val_dataset.select(range(100)),\n",
    "    \n",
    "#     callbacks=[LogCallback()]                   # *** to print the logs in the terminal\n",
    "# )\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# # for training on the 5 different datasets mixed by proportions specified for each of the batch\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset= None,                     # overridden\n",
    "#     eval_dataset= tokenized_val_dataset,\n",
    "#     callbacks=[LogCallback()]\n",
    "# )\n",
    "\n",
    "# # Override the train dataloader\n",
    "# trainer.get_train_dataloader = lambda: ProportionalDataLoader(\n",
    "#     datasets_dict,\n",
    "#     proportions,\n",
    "#     training_args.per_device_train_batch_size\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a367c20-fa53-4e15-b2b2-fa8a4da8b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 328\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1,230\n",
      "  Number of trainable parameters = 737,668,096\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 230/1230 04:55 < 21:37, 0.77 it/s, Epoch 5.59/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>30.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>27.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>25.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>22.752200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>20.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>16.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>9.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.486400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: {'loss': 30.3158, 'grad_norm': 81.26324462890625, 'learning_rate': 9e-06, 'epoch': 0.24390243902439024}\n",
      "Step 20: {'loss': 27.961, 'grad_norm': 77.45418548583984, 'learning_rate': 9.926229508196722e-06, 'epoch': 0.4878048780487805}\n",
      "Step 30: {'loss': 25.3641, 'grad_norm': 138.82952880859375, 'learning_rate': 9.844262295081968e-06, 'epoch': 0.7317073170731707}\n",
      "Step 40: {'loss': 22.7522, 'grad_norm': 390.9015808105469, 'learning_rate': 9.762295081967213e-06, 'epoch': 0.975609756097561}\n",
      "Step 50: {'loss': 20.638, 'grad_norm': 58.933746337890625, 'learning_rate': 9.68032786885246e-06, 'epoch': 1.2195121951219512}\n",
      "Step 60: {'loss': 16.3215, 'grad_norm': 100.96739959716797, 'learning_rate': 9.598360655737707e-06, 'epoch': 1.4634146341463414}\n",
      "Step 70: {'loss': 9.9247, 'grad_norm': 67.94738006591797, 'learning_rate': 9.516393442622952e-06, 'epoch': 1.7073170731707317}\n",
      "Step 80: {'loss': 6.1906, 'grad_norm': 41.0464973449707, 'learning_rate': 9.434426229508199e-06, 'epoch': 1.951219512195122}\n",
      "Step 90: {'loss': 3.421, 'grad_norm': 30.133033752441406, 'learning_rate': 9.352459016393444e-06, 'epoch': 2.1951219512195124}\n",
      "Step 100: {'loss': 1.8583, 'grad_norm': 23.43144989013672, 'learning_rate': 9.270491803278689e-06, 'epoch': 2.4390243902439024}\n",
      "Step 110: {'loss': 1.2689, 'grad_norm': 9.687514305114746, 'learning_rate': 9.188524590163935e-06, 'epoch': 2.682926829268293}\n",
      "Step 120: {'loss': 0.7611, 'grad_norm': 11.101236343383789, 'learning_rate': 9.10655737704918e-06, 'epoch': 2.926829268292683}\n",
      "Step 130: {'loss': 0.5542, 'grad_norm': 5.589099407196045, 'learning_rate': 9.024590163934427e-06, 'epoch': 3.1707317073170733}\n",
      "Step 140: {'loss': 0.4864, 'grad_norm': 3.2348830699920654, 'learning_rate': 8.942622950819672e-06, 'epoch': 3.4146341463414633}\n",
      "Step 150: {'loss': 0.4383, 'grad_norm': 1.523375153541565, 'learning_rate': 8.860655737704919e-06, 'epoch': 3.658536585365854}\n",
      "Step 160: {'loss': 0.354, 'grad_norm': 1.144379734992981, 'learning_rate': 8.778688524590164e-06, 'epoch': 3.902439024390244}\n",
      "Step 170: {'loss': 0.3265, 'grad_norm': 0.9550507664680481, 'learning_rate': 8.69672131147541e-06, 'epoch': 4.146341463414634}\n",
      "Step 180: {'loss': 0.3042, 'grad_norm': 1.4252870082855225, 'learning_rate': 8.614754098360656e-06, 'epoch': 4.390243902439025}\n",
      "Step 190: {'loss': 0.2798, 'grad_norm': 0.8811467289924622, 'learning_rate': 8.532786885245903e-06, 'epoch': 4.634146341463414}\n",
      "Step 200: {'loss': 0.2486, 'grad_norm': 0.8813831806182861, 'learning_rate': 8.45081967213115e-06, 'epoch': 4.878048780487805}\n",
      "Step 210: {'loss': 0.2463, 'grad_norm': 0.8718535304069519, 'learning_rate': 8.368852459016394e-06, 'epoch': 5.121951219512195}\n",
      "Step 220: {'loss': 0.2251, 'grad_norm': 1.0943152904510498, 'learning_rate': 8.28688524590164e-06, 'epoch': 5.365853658536586}\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(save_path)                           \n",
    "\n",
    "tokenizer.save_pretrained(save_path)                    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166932f-d6e2-428b-a237-897e3c86d188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a5925-4845-4861-bebb-011647f358d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f478ac-861b-4415-9ca5-d671816b91be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bdc5c-d925-450f-8329-24a161a460a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d61cca-3e83-42f5-86a7-62e1087c3519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7840e092-7536-4fae-b74d-85ab8e2a435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the finetuned model name\n",
    "new_model = \"./T5large_Question_Generation\"\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(new_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a2ceb7-4950-4e76-b9cb-94c9d6ae66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the finetuned question generation model with a sample context\n",
    "\n",
    "\n",
    "def get_question(tag, difficulty, context, answer=\"\", num_questions=3, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate questions using the fine-tuned T5 model.\n",
    "    \n",
    "    Parameters:\n",
    "    - tag: Type of question (e.g., \"short answer\", \"multiple choice question\", \"true or false question\")\n",
    "    - difficulty: \"easy\", \"medium\", \"hard\"\n",
    "    - context: Supporting context or passage\n",
    "    - answer: Optional — if you want targeted question generation\n",
    "    - num_questions: Number of diverse questions to generate\n",
    "    - max_length: Max token length of generated output\n",
    "    \n",
    "    Returns:\n",
    "    - List of generated questions as strings\n",
    "    \"\"\"\n",
    "    # Format input text based on whether answer is provided\n",
    "    answer_part = f\"[{answer}]\" if answer else \"\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{answer_part} {context}\"\n",
    "\n",
    "    # Tokenize\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    # Generate questions\n",
    "    output = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_questions,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    # Decode generated questions\n",
    "    return [tokenizer.decode(out, skip_special_tokens=True) for out in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0cdca68-49a0-4884-8c1b-a724d6fc304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the chemical formula of the ethyl alcohol that gives rise to the chemical name alcohol?', 'What is the chemical formula of ethanol?; answer:[C2H5O]; wrong options:(Water, Acid, Air, Food, Fuel, etc.)', 'What is the chemical formula of alcohol?; answer:[Ethyl alcohol]; wrong options:(Alcohol; Water; Food)']\n"
     ]
    }
   ],
   "source": [
    "# short answer question\n",
    "print(get_question(\n",
    "    tag=\"short answer\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"Cadmium chloride is a hygroscopic solid that is highly soluble in water and slightly soluble in alcohol. Ethanol, also called alcohol, ethyl alcohol, and drinking alcohol, is a compound and simple alcohol with the chemical formula C2H5OH.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409aab2c-fa28-47c6-82d9-041c9ed85b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is a common misconception about ethanol?; answer:[false]', 'Alcohol is used to help prevent drunkenness and to fight crime.; answer: [true]', 'How do various different types of ethanol differ in terms of their properties?']\n",
      "['Ethanol is a type of sugar.; answer: [false]', 'The amount of ethanol produced by the US is quite small in comparison to that of the European Union.; answer: [true]', 'How does alcohol differ from ethanol?']\n"
     ]
    }
   ],
   "source": [
    "# true or false question\n",
    "print(get_question(\n",
    "    tag=\"true or false question\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"Ethanol, also known as drinking alcohol, is a clear, colorless liquid that is flammable and is produced by the fermentation of sugars by yeast. It has the chemical formula C2H5OH and is used both recreationally and industrially.\"\n",
    "))\n",
    "\n",
    "print(get_question(\n",
    "    tag=\"true or false question\",\n",
    "    difficulty=\"easy\",\n",
    "    context=\"Ethanol, also known as drinking alcohol.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46873527-532a-4961-94f9-f0d056eaab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is a chemical reaction of ethanol?; answer:[Volatile fluid, flammable, characteristic odor]; wrong options:(Material, cooking; Fuel additive)', 'What uses are made of ethanol?; answer:[Recreational beverage]; wrong options:(Fuel additive; Fuel additive)', 'The basic chemical formula is C2H5.; answer: [false]']\n"
     ]
    }
   ],
   "source": [
    "# multiple choice question\n",
    "print(get_question(\n",
    "    tag=\"multiple choice question\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"Ethanol is used as a recreational beverage, as a solvent, and as a fuel additive. It is a volatile, flammable, colorless liquid with a slight characteristic odor, and its chemical formula is C2H5OH.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00300d22-e260-42eb-a585-2a7cd1bbd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is a computer?', 'What is a computer?; answer:[Input]; wrong options:(Survey data; Record data; Store data)', 'What is a computer?; answer:[Really, a computer]; wrong options:(Really, a computer; Storage capacity; Operation speed)']\n"
     ]
    }
   ],
   "source": [
    "# short answer question\n",
    "print(get_question(\n",
    "    tag=\"short answer\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"A computer is an electronic device that takes in data (input), processes it using hardware and software, stores it, and then produces information (output). At its core, it manipulates binary code (1s and 0s) through transistors and logic gates, performing complex tasks incredibly fast, making it efficient for everything from simple calculations to running complex applications\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53e1c8-d2bb-46ac-93b1-1f6eeda0e03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
