{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34648e71-74f3-4bd5-9449-e7008f1b9c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - /home/aalla4/SML\n"
     ]
    }
   ],
   "source": [
    "# setting up the directory where we want to store the models\n",
    "import os\n",
    "\n",
    "print(\"Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - \" + os.getcwd())\n",
    "os.environ['HF_HOME'] = \"/home/aalla4\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/home/aalla4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b0911c-3a5f-49c6-8d0a-6dfbcc20b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 20:00:09.343708: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of logical cores = 48\n",
      "CUDA available:  True\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import T5Config\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# getting the CPU and GPU count\n",
    "\n",
    "print(\"Total number of logical cores = \" + str(os.cpu_count()))  # This shows logical cores not the physical cores\n",
    "LOGICAL_CORES = os.cpu_count()\n",
    "USABLE_CPU_CORES = LOGICAL_CORES - 1    # YOU CAN CHANGE THIS ACCORDING TO THE CPU AVAILABILITIES\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65439bfe-c6a6-4f28-8091-80c72c115f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max positional embeddings supported by model - t5-base:  512\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer \n",
    "\n",
    "model_name = \"t5-base\" \n",
    "\n",
    "save_path = \"./T5base_Question_Generation\"\n",
    "\n",
    "config = T5Config.from_pretrained(\n",
    "    model_name,\n",
    "    dropout_rate=0.1,                 # encoder/decoder FFN dropout\n",
    "    attention_dropout_rate=0.1,       # self-attention dropout\n",
    ")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "\n",
    "print(f\"Max positional embeddings supported by model - {model_name}: \", model.config.n_positions)\n",
    "\n",
    "# tokenizer_input_max_length = 512\n",
    "# tokenizer_label_max_length = 250\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e69fcb-c7a9-4010-9eba-e2b5abf21ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation of special tokens used:\n",
      "- <extra_id_99> is used as a placeholder for [CONTEXT]\n",
      "- <extra_id_98> is used as a placeholder for [DIFFICULTY]\n",
      "- <extra_id_97> is used as a placeholder for [TAG]\n"
     ]
    }
   ],
   "source": [
    "# Making some special tokens as placeholders/seperators for the input \n",
    "\n",
    "# we don't waste any additional tokens in this process to seperate the inputs into sections\n",
    "special_tokens_description = {\n",
    "    \"<extra_id_99>\": \"[CONTEXT]\",      # Represents the context section of the input\n",
    "    \"<extra_id_98>\": \"[DIFFICULTY]\",   # Represents the difficulty section of the input - 'easy'/ 'medium'/ 'hard'\n",
    "    \"<extra_id_97>\": \"[TAG]\"           # Represents the tag section of the input\n",
    "}\n",
    "\n",
    "print(\"Explanation of special tokens used:\")\n",
    "for token, description in special_tokens_description.items():\n",
    "    print(f\"- {token} is used as a placeholder for {description}\")\n",
    "\n",
    "# if we can simply use the existing tokens we don't wanna increase additional special tokens\n",
    "# special_tokens = {'additional_special_tokens': ['[CONTEXT]', '[DIFFICULTY]', '[TAG]']}\n",
    "\n",
    "# Get current additional special tokens from the tokenizer\n",
    "# existing_special_tokens = tokenizer.special_tokens_map.get('additional_special_tokens', [])\n",
    "\n",
    "# tokenizer.add_special_tokens(special_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69515e-2bf2-462e-ab49-35079b2073bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b2ca8-9bf1-465d-9472-fa6739b53823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeaa029-4650-4358-a891-40908dd881f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# loading the custom datasets\n",
    "# Loading the dataset\n",
    "# Function to load the data from a JSON file\n",
    "\n",
    "filepath_descriptive = \"./dataset/descriptive.json\" \n",
    "filepath_mcq = \"./dataset/mcq.json\" \n",
    "filepath_tf = \"./dataset/true_false.json\" \n",
    "\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)                    # This should be a list of dicts\n",
    "\n",
    "        \n",
    "descriptive_data = load_json_file(filepath_descriptive)\n",
    "mcq_data = load_json_file(filepath_mcq)\n",
    "tf_data = load_json_file(filepath_tf)\n",
    "\n",
    "# Convert list of dicts to Hugging Face Dataset\n",
    "dataset_descriptive = Dataset.from_list(descriptive_data)\n",
    "dataset_mcq = Dataset.from_list(mcq_data)\n",
    "dataset_tf = Dataset.from_list(tf_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f95c8b-493a-40b4-85fe-16dc7c8a10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps\n",
    "\n",
    "# Preprocessing function to tokenize the input and target text\n",
    "def preprocess_descriptive(example):\n",
    "    '''<extra_id_97>short answer question <extra_id_98>easy <extra_id_99>Drinking enough water each day helps regulate body temperature, keep joints lubricated, prevent infections, and keep organs functioning properly. Proper hydration also improves sleep quality, cognition, and mood. \n",
    "       List two ways drinking water benefits the human body.'''\n",
    "    tag = example[\"tag\"]\n",
    "    difficulty = example[\"difficulty\"]\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = question\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_mcq(example):\n",
    "    '''there has been a mistake in the dataset so we have considered all the mcq to be of medium difficulty'''\n",
    "    '''<extra_id_97>multiple choice question <extra_id_98>medium <extra_id_99>Rainforests are essential to Earth’s ecosystem. They produce oxygen, absorb carbon dioxide, and help regulate the global climate. Rainforests are also home to more than half of the world’s plant and animal species. Despite their importance, they are being destroyed at an alarming rate due to logging, agriculture, and mining. When rainforests are cleared, biodiversity is lost, and carbon is released into the atmosphere, contributing to global warming. Indigenous people who depend on these forests are also displaced. Preserving rainforests is vital for maintaining environmental balance and protecting wildlife. \n",
    "       Which of the following is a consequence of rainforest destruction? [C. Global warming] (A. Improved biodiversity; B. Carbon absorption; C. Global warming; D. Increased rainfall)'''\n",
    "    tag = example[\"difficulty\"]            # there is a problem with the dataset so had to keep it like this\n",
    "    difficulty = \"medium\"                  # there is a problem with the dataset so had to keep this like it\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    options = example[\"options\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    # Prepare formatted options\n",
    "    # option_labels = ['A', 'B', 'C', 'D']\n",
    "    # formatted_options = [f\"{label}. {opt}\" for label, opt in zip(option_labels, options)]\n",
    "    # correct_index = options.index(answer)\n",
    "    # correct_option = f\"{option_labels[correct_index]}. {answer}\"\n",
    "\n",
    "    # don't use any options here as this might confuse the model; instead just give the answer and other options\n",
    "    # Get ONLY the other options (no A/B/C/D labels)\n",
    "    formatted_options = [opt for opt in options if opt != answer]\n",
    "    correct_option = f\"{answer}\"\n",
    "\n",
    "    # Prepare raw input and label strings\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = f\"{question}; answer:[{correct_option}]; wrong options:({'; '.join(formatted_options)})\"\n",
    "\n",
    "    # Tokenize input and target\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Add labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_tf(example):\n",
    "    '''<extra_id_97>true or false question <extra_id_98>easy <extra_id_99>Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water. The process typically occurs in the chloroplasts of plant cells and releases oxygen as a byproduct. Chlorophyll, the green pigment in plants, plays a crucial role in capturing light energy. This energy is then used to convert water and carbon dioxide into glucose, which serves as the plant’s food source. \n",
    "        Photosynthesis releases oxygen as a byproduct. [true]'''\n",
    "    tag = example[\"tag\"]\n",
    "    difficulty = example[\"difficulty\"]\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"].lower()  # ensure it's \"true\" or \"false\"\n",
    "\n",
    "    # Format the input and target\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = f\"{question}; answer: [{answer}]\"\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596aeff4-5397-4946-9626-a95f46282166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_descriptive_dataset = dataset_descriptive.map(preprocess_descriptive, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=dataset_descriptive.column_names)\n",
    "processed_mcq_dataset = dataset_mcq.map(preprocess_mcq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=dataset_mcq.column_names)\n",
    "processed_tf_dataset = dataset_tf.map(preprocess_tf, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=dataset_tf.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4774557-e76c-4e09-9836-fced0bb8b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processed_descriptive_dataset)\n",
    "# print(processed_mcq_dataset)\n",
    "# print(processed_tf_dataset)\n",
    "\n",
    "tokenized_train_dataset = concatenate_datasets([\n",
    "    processed_descriptive_dataset,\n",
    "    processed_mcq_dataset,\n",
    "    processed_tf_dataset\n",
    "]).shuffle(seed=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f760eaa-7a3d-4989-8540-fb2c616edd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving the custom dataset in a parquet format\n",
    "\n",
    "tokenized_trainset_save_path = \"./dataset/tokenized_custom_trainset.parquet\"\n",
    "\n",
    "tokenized_train_dataset.to_parquet(tokenized_trainset_save_path)\n",
    "\n",
    "print(f\" Saved the custom training dataset to {tokenized_trainset_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4484e-807f-405c-8fcb-413cfb1d6452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d561ea-de24-4b3c-be0c-2b0ab47fd148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ae5eff-f995-4a3e-af60-6d13fbd34faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the 5 mixed datasets - not according to proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d87b924-8222-44ba-9a67-e2517abd4476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets ........\n",
      "Completed loading the datasets ........\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "# datasets used are squad v1, hotpotqa, openbookqa, drop, boolq\n",
    "# the datasets returns a DatasetDict with \"train\" and \"validation\" splits\n",
    "\n",
    "print(\"Loading the datasets ........\")\n",
    "\n",
    "# Load HotpotQA (distractor)\n",
    "hotpotqa = load_dataset(\"hotpot_qa\", \"distractor\",  trust_remote_code=True)\n",
    "\n",
    "# SQUAD (V1)\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# OpenBookQA (additional)\n",
    "openbookqa = load_dataset(\"openbookqa\", \"additional\")\n",
    "\n",
    "# Boolq dataset\n",
    "boolq = load_dataset(\"boolq\")\n",
    "\n",
    "# Drop dataset\n",
    "drop = load_dataset(\"drop\")\n",
    "\n",
    "print(\"Completed loading the datasets ........\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35050736-2ab1-4836-9813-fddc90175d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preprocesing; Available CPU Cores to use - 47\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing functions of the datasets\n",
    "\n",
    "# Preprocessing, combining and shuffling datasets only uses LOGICAL_CORES/ CPU's and not done on GPU's\n",
    "print(\"Starting Preprocesing; Available CPU Cores to use - \"+ str(USABLE_CPU_CORES))\n",
    "\n",
    "def preprocess_hotpotqa(example):  \n",
    "    \"hotpotq - used for short answer question generation, with focus on multi-hop sentences rater than forming the question with just a single line in the whole context\"      \n",
    "\n",
    "    answer = example['answer']\n",
    "    if len(answer.split()) < 12:                    \n",
    "        tag = \"very short answer\"\n",
    "    else:\n",
    "        tag = \"short answer\"\n",
    "    difficulty = example['level']                   # Extract difficulty - 'easy'/ 'medium'/ 'hard'\n",
    "\n",
    "    # thought of just keeping the supporting sentences instead of the all the sentences in the supporting titles, but the context length is too short for this approach           \n",
    "    supporting_facts = example[\"supporting_facts\"]\n",
    "    supporting_titles = set([t for t in supporting_facts['title']])          # Use set to avoid duplicates\n",
    "    context_titles = example['context']['title']\n",
    "    context_sentences = example['context']['sentences']\n",
    "\n",
    "    supporting_sentences = []\n",
    "\n",
    "    for idx, title in enumerate(context_titles):\n",
    "        if title in supporting_titles:\n",
    "            # Add all sentences under this title\n",
    "            req_sentences = []\n",
    "            for sent in context_sentences[idx]:\n",
    "                req_sentences.append(f\"{sent}\")\n",
    "            sentence_block = \" \".join(req_sentences)  \n",
    "            supporting_sentences.append(f\"{title}, {sentence_block}\")\n",
    "\n",
    "    short_context = \" \".join(supporting_sentences)\n",
    "    \n",
    "    # Randomly decide whether to include the answer or not as not always the answer will be likely to be provided by the instructor\n",
    "    include_answer = random.choices([True, False], weights=[15, 85], k=1)[0]\n",
    "\n",
    "    if include_answer:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>[{answer}] {short_context}\"     # Prepare the model input\n",
    "    else:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{short_context}\"\n",
    "       \n",
    "    target_text = f\"{example['question']}\"                                                                       # Prepare the target output\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text) \n",
    "    \n",
    "    # Tokenize both input and output\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Attach labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_squad(example):\n",
    "    \"squad - used for short answer question generation, with multiple questions generated with the same context\"\n",
    "    \n",
    "    answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "    if len(answer.split()) < 12:\n",
    "        tag = \"very short answer\"\n",
    "    else:\n",
    "        tag = \"short answer\"\n",
    "    difficulty = \"easy\"\n",
    "    context = example['context']\n",
    "\n",
    "    include_answer = random.choices([True, False], weights=[15, 85], k=1)[0]\n",
    "    if include_answer:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>[{answer}] {context}\"\n",
    "    else:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "\n",
    "    target_text = example[\"question\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_openbookqa(example):\n",
    "    \"openbookqa - used for multiple choice question generation\"\n",
    "   \n",
    "    # difficulty is calculated based on the human score and clarity\n",
    "    clarity = example.get(\"clarity\", 0)\n",
    "    human_score = example.get(\"human_score\", 1)\n",
    "    \n",
    "    # Filter out low clarity examples\n",
    "    # if clarity <= 1:\n",
    "    #     return None       # discard low-clarity / low-quality examples\n",
    "        # return {}\n",
    "\n",
    "    # Assign difficulty\n",
    "    if clarity > 1.8 and human_score < 1:\n",
    "        difficulty = \"hard\" \n",
    "    else:                             # [ 1 < clarity <= 1.8 ] and if [ clarity > 1.8 and human_score > 1 ]\n",
    "        difficulty = \"medium\"\n",
    "\n",
    "    tag = \"multiple choice question\"\n",
    "    \n",
    "    # Build the context using fact1 (from additional)\n",
    "    fact1 = example.get(\"fact1\", \"\")                # example.get(\"fact1\", \"\") is safe — it gives you a default value if the key is missing, example[\"fact1\"] will raise a KeyError if 'fact1' is missing.\n",
    "    \n",
    "    # Construct the multiple choice question format with answer marked\n",
    "    question_stem = example[\"question_stem\"]\n",
    "    answer_key = example[\"answerKey\"]  # 'A', 'B', 'C', 'D'\n",
    "    choices = example[\"choices\"]       # dict with \"text\" and \"label\"\n",
    "    \n",
    "    # Extract choice texts\n",
    "    choices_text = choices[\"text\"]\n",
    "    \n",
    "    # Get index of correct answer\n",
    "    idx = ord(answer_key) - ord('A')\n",
    "    # Correct answer text\n",
    "    answer_text = choices_text[idx]\n",
    "    \n",
    "    # Remove correct answer\n",
    "    other_options = [opt for i, opt in enumerate(choices_text) if i != idx]\n",
    "    choices_str = \"; \".join(other_options)\n",
    "\n",
    "    \n",
    "    target_text = f\"{question_stem}; answer:[{answer_text}]; wrong options:({choices_str})\"\n",
    "\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{fact1}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"clarity\"] = clarity                 # Add clarity for later filtering\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_boolq(example):\n",
    "    \"boolq - used for true/ false question generation\"\n",
    "\n",
    "    # difficulty tagging is not available in BoolQ, so we default it\n",
    "    difficulty = \"medium\"\n",
    "    tag = \"true or false question\"\n",
    "\n",
    "    # Extract question and answer\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]          # this is a bool value: True / False\n",
    "    passage = example[\"passage\"]\n",
    "\n",
    "    target_text = f\"{question}; answer:[{'true' if answer else 'false'}]\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{passage}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_drop(example):\n",
    "    \"drop - used for very short answer question generation, with multiple questions generated with the same context, it  also includes some math calculations that has to be done by the model\"\n",
    "\n",
    "    tag = \"one word answer\"\n",
    "    difficulty = \"medium\"\n",
    "    question = example.get(\"question\", \"\")\n",
    "    passage = example.get(\"passage\", \"\")\n",
    "    \n",
    "    # Retrieve the first available answer from 'answers_spans' (string or list of strings)\n",
    "    answers = example.get(\"answers_spans\", {}).get(\"spans\", [])\n",
    "    if not answers:\n",
    "        return None                                           # Skip if no answer available\n",
    "\n",
    "    answer = answers[0]      \n",
    "    \n",
    "    target_text = f\"{question}; answer:[{answer}]\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{passage}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11765b5-0465-4dce-9d2f-f286131ebf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into train and validation & Preprocessing the datasets individually\n",
    "\n",
    "# Preprocess both splits separately for both datasets\n",
    "# put the .select(range(5)) steps for testing out the input and output results and printing them ******\n",
    "# hotpotqa_train = hotpotqa[\"train\"].select(range(5)).map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES)\n",
    "# hotpotqa_train = hotpotqa[\"train\"].select(range(5)).map(preprocess_hotpotqa)\n",
    "\n",
    "# trail\n",
    "# drop_train = drop[\"train\"].select(range(5)).map(preprocess_drop)\n",
    "# drop_val = drop[\"validation\"].select(range(5)).map(preprocess_drop)\n",
    "# boolq_train = boolq[\"train\"].select(range(5)).map(preprocess_boolq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be10f0f-08ec-484f-91f5-69b9e237ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_train = hotpotqa[\"train\"].map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=hotpotqa[\"train\"].column_names)\n",
    "hotpotqa_val = hotpotqa[\"validation\"].map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=hotpotqa[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae231b0f-2b4c-4ea6-8116-122808334ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = squad[\"train\"].map(preprocess_squad, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=squad[\"train\"].column_names)\n",
    "squad_val = squad[\"validation\"].map(preprocess_squad, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=squad[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f027d0a7-ef2d-4b42-9e9f-c54748ceb9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c007dbedc09406ea41d20c0bff3b457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=47):   0%|          | 0/4957 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e649f093314ab6b6ba46726f734a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4957 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cd58b8512f42ae8b316f9db98361db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=47):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbd95c9a0e24f96a25b3b183fcef0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openbookqa_train = openbookqa[\"train\"].map(preprocess_openbookqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=openbookqa[\"train\"].column_names).filter( lambda x: (x.get(\"clarity\", 0) > 1)).remove_columns([\"clarity\"])\n",
    "openbookqa_val = openbookqa[\"validation\"].map(preprocess_openbookqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=openbookqa[\"validation\"].column_names).filter( lambda x: (x.get(\"clarity\", 0) > 1)).remove_columns([\"clarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c759676d-4f99-46dd-ae3a-6aea0b18d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_train = boolq[\"train\"].map(preprocess_boolq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=boolq[\"train\"].column_names)\n",
    "boolq_val = boolq[\"validation\"].map(preprocess_boolq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=boolq[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7794b9a9-32bd-43b8-8f1e-958754bb57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = drop[\"train\"].map(preprocess_drop, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=drop[\"train\"].column_names)\n",
    "drop_val = drop[\"validation\"].map(preprocess_drop, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=drop[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9dafec-06f2-44c1-8f8c-a3a123f1e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the datasets and shuffling them\n",
    "\n",
    "# Combine and shuffle the datasets\n",
    "tokenized_train_dataset = concatenate_datasets([\n",
    "    hotpotqa_train,\n",
    "    squad_train,\n",
    "    openbookqa_train,\n",
    "    boolq_train,\n",
    "    drop_train\n",
    "]).shuffle(seed=42)\n",
    "\n",
    "tokenized_val_dataset = concatenate_datasets([\n",
    "    hotpotqa_val,\n",
    "    squad_val,\n",
    "    openbookqa_val,\n",
    "    boolq_val,\n",
    "    drop_val\n",
    "# ])                                        # no need to shuffle the validation dataset...\n",
    "]).shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e68465c2-f3a3-4079-b708-8e046ccd6f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be8b81534fe46c49484300fc6b517c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/270 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76bd0385f514dfa81e1cf3ad88c588e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/32 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "142916148"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to Parquet (efficient for large datasets)\n",
    "\n",
    "tokenized_trainset_save_path = \"./dataset/tokenized_trainset.parquet\"\n",
    "tokenized_valset_save_path = \"./dataset/tokenized_valset.parquet\"\n",
    "\n",
    "\n",
    "tokenized_train_dataset.to_parquet(tokenized_trainset_save_path)\n",
    "tokenized_val_dataset.to_parquet(tokenized_valset_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64170b3c-2dd2-4611-b138-b10e825843d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184129b-0f77-4f18-a881-d85e96956e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adfd04-9727-48a2-a791-2c459a65cd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca887136-64df-49c4-b6e5-33893c75fa87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359be085-8c49-40fa-a4be-5911e1175610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf6a3f-a7f0-4d42-b9d6-d027b3c32b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
