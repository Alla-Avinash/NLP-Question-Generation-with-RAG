{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac72cd29-d098-432a-9e7c-14f787f981e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - /home/aalla4/SML\n"
     ]
    }
   ],
   "source": [
    "# setting up the directory where we want to store the models\n",
    "import os\n",
    "\n",
    "print(\"Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - \" + os.getcwd())\n",
    "os.environ['HF_HOME'] = \"/home/aalla4\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/home/aalla4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38daf9f-e838-4656-84a2-755195ed1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 18:13:37.402286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of logical cores = 48\n",
      "CUDA available:  True\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import T5Config\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# getting the CPU and GPU count\n",
    "\n",
    "print(\"Total number of logical cores = \" + str(os.cpu_count()))  # This shows logical cores not the physical cores\n",
    "LOGICAL_CORES = os.cpu_count()\n",
    "USABLE_CPU_CORES = LOGICAL_CORES - 1    # YOU CAN CHANGE THIS ACCORDING TO THE CPU AVAILABILITIES\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d415bd-c660-4ec2-bea8-f534244558e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer \n",
    "\n",
    "# model_name = \"t5-base\" # ---------------------------------------------------------------------------------------------\n",
    "# model_name = \"t5-large\" \n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f11fb4-20de-40fd-ace5-8c4246586a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation of special tokens used:\n",
      "- <extra_id_99> is used as a placeholder for [CONTEXT]\n",
      "- <extra_id_98> is used as a placeholder for [DIFFICULTY]\n",
      "- <extra_id_97> is used as a placeholder for [TAG]\n"
     ]
    }
   ],
   "source": [
    "# Making some special tokens as placeholders/seperators for the input \n",
    "\n",
    "# we don't waste any additional tokens in this process to seperate the inputs into sections\n",
    "special_tokens_description = {\n",
    "    \"<extra_id_99>\": \"[CONTEXT]\",      # Represents the context section of the input\n",
    "    \"<extra_id_98>\": \"[DIFFICULTY]\",   # Represents the difficulty section of the input - 'easy'/ 'medium'/ 'hard'\n",
    "    \"<extra_id_97>\": \"[TAG]\"           # Represents the tag section of the input\n",
    "}\n",
    "\n",
    "print(\"Explanation of special tokens used:\")\n",
    "for token, description in special_tokens_description.items():\n",
    "    print(f\"- {token} is used as a placeholder for {description}\")\n",
    "\n",
    "# if we can simply use the existing tokens we don't wanna increase additional special tokens\n",
    "# special_tokens = {'additional_special_tokens': ['[CONTEXT]', '[DIFFICULTY]', '[TAG]']}\n",
    "\n",
    "# Get current additional special tokens from the tokenizer\n",
    "# existing_special_tokens = tokenizer.special_tokens_map.get('additional_special_tokens', [])\n",
    "\n",
    "# tokenizer.add_special_tokens(special_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f600c90-163c-4cc7-ba58-85e5da63aa67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1142592c-8e5c-4ee8-ae48-9965d12f19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed loading the datasets ........\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# loading the custom datasets\n",
    "# Loading the dataset\n",
    "# Function to load the data from a JSON file\n",
    "\n",
    "filepath_descriptive = \"./dataset/descriptive.json\" \n",
    "filepath_mcq = \"./dataset/mcq.json\" \n",
    "filepath_tf = \"./dataset/true_false.json\" \n",
    "\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)                    # This should be a list of dicts\n",
    "\n",
    "        \n",
    "descriptive_data = load_json_file(filepath_descriptive)\n",
    "mcq_data = load_json_file(filepath_mcq)\n",
    "tf_data = load_json_file(filepath_tf)\n",
    "\n",
    "# Convert list of dicts to Hugging Face Dataset\n",
    "dataset_descriptive = Dataset.from_list(descriptive_data)\n",
    "dataset_mcq = Dataset.from_list(mcq_data)\n",
    "dataset_tf = Dataset.from_list(tf_data)\n",
    "\n",
    "\n",
    "print(\"Completed loading the datasets ........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b201a278-e261-4266-9595-2656d5270af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps\n",
    "\n",
    "# Preprocessing function to tokenize the input and target text\n",
    "def preprocess_descriptive(example):\n",
    "    '''<extra_id_97>short answer question <extra_id_98>easy <extra_id_99>Drinking enough water each day helps regulate body temperature, keep joints lubricated, prevent infections, and keep organs functioning properly. Proper hydration also improves sleep quality, cognition, and mood. \n",
    "       List two ways drinking water benefits the human body.'''\n",
    "    tag = example[\"tag\"]\n",
    "    difficulty = example[\"difficulty\"]\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    input_text = f\"<extra_id_97>descriptive question ({tag}) <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = question\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_mcq(example):\n",
    "    '''there has been a mistake in the dataset so we have considered all the mcq to be of medium difficulty'''\n",
    "    '''<extra_id_97>multiple choice question <extra_id_98>medium <extra_id_99>Rainforests are essential to Earth’s ecosystem. They produce oxygen, absorb carbon dioxide, and help regulate the global climate. Rainforests are also home to more than half of the world’s plant and animal species. Despite their importance, they are being destroyed at an alarming rate due to logging, agriculture, and mining. When rainforests are cleared, biodiversity is lost, and carbon is released into the atmosphere, contributing to global warming. Indigenous people who depend on these forests are also displaced. Preserving rainforests is vital for maintaining environmental balance and protecting wildlife. \n",
    "       Which of the following is a consequence of rainforest destruction? [C. Global warming] (A. Improved biodiversity; B. Carbon absorption; C. Global warming; D. Increased rainfall)'''\n",
    "    tag = example[\"difficulty\"]            # there is a problem with the dataset so had to keep it like this\n",
    "    difficulty = \"medium\"                  # there is a problem with the dataset so had to keep this like it\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    options = example[\"options\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    # Prepare formatted options\n",
    "    # option_labels = ['A', 'B', 'C', 'D']\n",
    "    # formatted_options = [f\"{label}. {opt}\" for label, opt in zip(option_labels, options)]\n",
    "    # correct_index = options.index(answer)\n",
    "    # correct_option = f\"{option_labels[correct_index]}. {answer}\"\n",
    "\n",
    "    # don't use any options here as this might confuse the model; instead just give the answer and other options\n",
    "    # Get ONLY the other options (no A/B/C/D labels)\n",
    "    formatted_options = [opt for opt in options if opt != answer]\n",
    "    correct_option = f\"{answer}\"\n",
    "\n",
    "    # Prepare raw input and label strings\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = f\"{question}; answer:[{correct_option}]; wrong options:({'; '.join(formatted_options)})\"\n",
    "\n",
    "    # Tokenize input and target\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Add labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_tf(example):\n",
    "    '''<extra_id_97>true or false question <extra_id_98>easy <extra_id_99>Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water. The process typically occurs in the chloroplasts of plant cells and releases oxygen as a byproduct. Chlorophyll, the green pigment in plants, plays a crucial role in capturing light energy. This energy is then used to convert water and carbon dioxide into glucose, which serves as the plant’s food source. \n",
    "        Photosynthesis releases oxygen as a byproduct. [true]'''\n",
    "    tag = example[\"tag\"]\n",
    "    difficulty = example[\"difficulty\"]\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"].lower()  # ensure it's \"true\" or \"false\"\n",
    "\n",
    "    # Format the input and target\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = f\"{question}; answer: [{answer}]\"\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f73199-0404-4a4c-a21a-e29cfcf49bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d3b6a4fed44706aed1096ed0f05215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=47):   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dbebcc83a545b7a58d1f081b61914e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=47):   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ba637c21f14eacba7bd4cb121d6d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=47):   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "processed_descriptive_dataset = dataset_descriptive.map(preprocess_descriptive, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=dataset_descriptive.column_names)\n",
    "processed_mcq_dataset = dataset_mcq.map(preprocess_mcq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=dataset_mcq.column_names)\n",
    "processed_tf_dataset = dataset_tf.map(preprocess_tf, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=dataset_tf.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1e594b-1c17-4fc7-b468-7b632c5929a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_descriptive_dataset = processed_descriptive_dataset.select(range(110))\n",
    "processed_mcq_dataset = processed_mcq_dataset.select(range(100))\n",
    "processed_tf_dataset = processed_tf_dataset.select(range(110))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf1674a-a17c-4f01-93f9-46a597e8e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "dataset = processed_descriptive_dataset\n",
    "print(len(dataset))\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4f9c63-4347-43a6-ba6a-cb0192b3446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "dataset = processed_mcq_dataset\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d863f1c-b786-4d29-9f92-f53911eaa628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "dataset = processed_tf_dataset\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a45b786-80ef-42be-9a81-331c8d2ef101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportionally sample per dataset every batch\n",
    "\n",
    "datasets_dict = {\n",
    "    \"descriptive\": processed_descriptive_dataset,\n",
    "    \"mcq\": processed_mcq_dataset,\n",
    "    \"tf\": processed_tf_dataset,\n",
    "}\n",
    "\n",
    "proportions = {\n",
    "    \"descriptive\": 11/32,\n",
    "    \"mcq\": 10/32,\n",
    "    \"tf\": 11/32    \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eac2a46-7e6b-4196-a1dd-c498e377a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "# class ProportionalBatchSampler(Sampler):\n",
    "#     def __init__(self, datasets_dict, proportions, batch_size):\n",
    "#         self.datasets_dict = datasets_dict\n",
    "#         self.proportions = proportions\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#         # Pre-calc: how many samples per dataset per batch\n",
    "#         self.samples_per_dataset = {\n",
    "#             k: max(1, int(batch_size * p))\n",
    "#             for k, p in proportions.items()\n",
    "#         }\n",
    "\n",
    "#         # Make sure total == batch_size\n",
    "#         diff = batch_size - sum(self.samples_per_dataset.values())\n",
    "#         if diff > 0:\n",
    "#             largest_key = max(self.samples_per_dataset, key=lambda x: self.samples_per_dataset[x])\n",
    "#             self.samples_per_dataset[largest_key] += diff\n",
    "\n",
    "#         # Convert dataset to list of indices\n",
    "#         self.index_pools = {\n",
    "#             k: list(range(len(ds)))\n",
    "#             for k, ds in datasets_dict.items()\n",
    "#         }\n",
    "\n",
    "#         # Shuffle each dataset's index pool\n",
    "#         for k in self.index_pools:\n",
    "#             random.shuffle(self.index_pools[k])\n",
    "\n",
    "#         # Total batches = smallest number of batches any dataset can support\n",
    "#         self.total_batches = min([\n",
    "#             len(v) // self.samples_per_dataset[k]\n",
    "#             for k, v in self.index_pools.items()\n",
    "#         ])\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for _ in range(self.total_batches):\n",
    "#             batch_indices = []\n",
    "#             for k in self.index_pools:\n",
    "#                 take = self.samples_per_dataset[k]\n",
    "#                 batch_indices.extend(self.index_pools[k][:take])\n",
    "#                 del self.index_pools[k][:take]\n",
    "#             random.shuffle(batch_indices)\n",
    "#             yield batch_indices\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.total_batches\n",
    "\n",
    "class ProportionalBatchSampler(Sampler):\n",
    "    def __init__(self, datasets_dict, proportions, batch_size):\n",
    "        self.datasets_dict = datasets_dict\n",
    "        self.proportions = proportions\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.samples_per_dataset = {\n",
    "            k: max(1, int(batch_size * p))\n",
    "            for k, p in proportions.items()\n",
    "        }\n",
    "\n",
    "        diff = batch_size - sum(self.samples_per_dataset.values())\n",
    "        if diff > 0:\n",
    "            largest = max(self.samples_per_dataset, key=self.samples_per_dataset.get)\n",
    "            self.samples_per_dataset[largest] += diff\n",
    "\n",
    "        self.full_index_pools = {\n",
    "            k: list(range(len(ds)))\n",
    "            for k, ds in datasets_dict.items()\n",
    "        }\n",
    "\n",
    "        self.index_pools = {\n",
    "            k: pool.copy()\n",
    "            for k, pool in self.full_index_pools.items()\n",
    "        }\n",
    "\n",
    "        for k in self.index_pools:\n",
    "            random.shuffle(self.index_pools[k])\n",
    "\n",
    "        # Trainer controls total steps, not us\n",
    "        self.total_batches = 10**12  # effectively infinite\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            batch = []\n",
    "\n",
    "            for k in self.index_pools:\n",
    "                take = self.samples_per_dataset[k]\n",
    "\n",
    "                if len(self.index_pools[k]) < take:\n",
    "                    # Refill + reshuffle\n",
    "                    self.index_pools[k] = self.full_index_pools[k].copy()\n",
    "                    random.shuffle(self.index_pools[k])\n",
    "\n",
    "                batch.extend(self.index_pools[k][:take])\n",
    "                del self.index_pools[k][:take]\n",
    "\n",
    "            random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_batches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aadc2985-1b0d-413e-aad4-977cc472a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ProportionalDataLoader(DataLoader):\n",
    "    def __init__(self, datasets_dict, proportions, batch_size):\n",
    "        self.datasets_dict = datasets_dict\n",
    "        merged_dataset = concatenate_datasets(list(datasets_dict.values()))\n",
    "        sampler = ProportionalBatchSampler(datasets_dict, proportions, batch_size)\n",
    "        super().__init__(\n",
    "            merged_dataset,\n",
    "            batch_sampler=sampler,\n",
    "            collate_fn=trainer.data_collator\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623c088-4ca9-401b-83b6-70173a3fba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5376e06-2eb5-4699-86f2-48ad5a185b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3909c27-c36d-4bbe-aa6c-2d6127cdbe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max positional embeddings supported by model - google/flan-t5-base:  512\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer \n",
    "\n",
    "# model_name = \"t5-base\"                            # -----------------------------------------------------------------------------------------------------------------\n",
    "# model_name = \"t5-large\" \\\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "\n",
    "save_path = \"./T5flanbase_Question_Generation\"           # ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "config = T5Config.from_pretrained(\n",
    "    model_name,\n",
    "    dropout_rate=0.1,           # encoder/decoder FFN dropout\n",
    "    attention_dropout_rate=0.1, # self-attention dropout\n",
    ")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config = config)\n",
    "\n",
    "print(f\"Max positional embeddings supported by model - {model_name}: \", model.config.n_positions)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394937d-f6e2-4d8b-99ef-a14bd8614d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a6d3d5d-074f-4775-9aa7-edc9a6af6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the dataset and Loading the Tokenized Dataset if Preprocessing is already done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97dc1947-0b6a-44a2-9485-4b0aca9e62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset paths - choosing the dataset\n",
    "\n",
    "\n",
    "#  ----------------------------------------------------------------------------------------------\n",
    "# Choose Training set\n",
    "\n",
    "# custom dataset \n",
    "# tokenized_trainset_path = \"./dataset/tokenized_custom_trainset.parquet\"\n",
    "\n",
    "# 5 datasets mixed\n",
    "# tokenized_trainset_path = \"./dataset/tokenized_trainset.parquet\"\n",
    "\n",
    "# 5 datasets mixed proportionally\n",
    "tokenized_trainset_path = \"using 3 datasets mixed proportionally\"\n",
    "\n",
    "# Load from Parquet ---------- uncomment this\n",
    "# tokenized_train_dataset = Dataset.from_parquet(tokenized_trainset_path)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# Validation set\n",
    "\n",
    "\n",
    "# you only use the val set when needed but it can always be loaded up from the already preprocessed parquet file\n",
    "# tokenized_valset_path = \"./dataset/tokenized_valset.parquet\"\n",
    "# tokenized_val_dataset = Dataset.from_parquet(tokenized_valset_path)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0f3e9-fc49-4c6c-aaa3-49d0aa254d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63825cc7-cbc8-4eec-ab36-a6bb2b9efa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using the dataset =  using 3 datasets mixed proportionally\n",
      " Using the below parameters for training:\n",
      " epochs = 20 \n",
      " batch size = 32 \n",
      " learning rate = 1e-05 \n",
      " warmup steps = 10 \n",
      " weight decay = 0.001\n",
      " logging steps = 10\n"
     ]
    }
   ],
   "source": [
    "# prarmeters to change when you want to test out and experiment with various values depending on the dataset which you are using\n",
    "\n",
    "num_of_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 1e-5\n",
    "warmup_steps = 10\n",
    "weight_decay = 1e-3\n",
    "\n",
    "# for the model saving strategy\n",
    "save_steps = 100\n",
    "\n",
    "logging_steps = 10\n",
    "\n",
    "print(f\" Using the dataset =  {tokenized_trainset_path}\\n Using the below parameters for training:\\n epochs = {num_of_epochs} \\n batch size = {batch_size} \\n learning rate = {learning_rate} \\n warmup steps = {warmup_steps} \\n weight decay = {weight_decay}\\n logging steps = {logging_steps}\")\n",
    "\n",
    "# print(proportions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51947aad-c9c1-456b-8065-8aac7b88cfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5b134cb-9696-4920-a6bc-887478a490e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the current device:  cuda\n",
      "Device updated to: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Defning the Training args \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"This is the current device: \", device)\n",
    "print(\"Device updated to:\", model.device)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,              # directory where the training logs, checkpoints, and evaluation results (like metrics) are saved during the training process   \n",
    "    \n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_of_epochs,                                     \n",
    "    weight_decay=1e-3,                          \n",
    "\n",
    "    # generally Batch size = per_device_train_batch_size * gradient_accumulation_steps * number of devices\n",
    "    # -----------------------------------------------------------------------\n",
    "    # per_device_train_batch_size=batch_size,                    # ****** if using GPU's = 16; if using CPU's = 1 or 2\n",
    "    # # gradient_accumulation_steps=1,                     # Increase this if you need to simulate larger batch sizes, without running into 'Out or Memory' errors when memory is limited\n",
    "    # per_device_eval_batch_size=batch_size,\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # this is for the t5 large model on the custom dataset\n",
    "    # per_device_train_batch_size = 2,\n",
    "    # gradient_accumulation_steps = 4,\n",
    "\n",
    "    # this is not useful for CPU based training as hugging face trainer handles multi-core utilization automatically based on the system configuration\n",
    "    dataloader_num_workers= USABLE_CPU_CORES,          # ****** for optimal use of CPU and not wasting GPU time [ this helps in loading the next batch of data into the VRAM ]\n",
    "\n",
    "    # Print validation loss every epoch\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # eval_strategy=\"epoch\",                                                                                                   # need to change this for every new experiment\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Print and logs the training loss of the training data\n",
    "    logging_strategy=\"steps\",   \n",
    "    logging_steps=logging_steps,                                   # ****** if using GPU = 100; if using CPU = 1 or 2 \n",
    "\n",
    "    # saves model at the end of every epoch\n",
    "    \n",
    "    # save_strategy=\"epoch\",  \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    \n",
    "    save_total_limit=1,\n",
    "    # save_total_limit=2,\n",
    "\n",
    "    # report_to=\"none\",  # Disable default logging\n",
    "    \n",
    "    logging_dir= save_path + \"/logs\",           # save logs to a directory\n",
    "    report_to=\"tensorboard\",                    # Reports to TensorBoard\n",
    "    log_level='info',                           # Set logging level to 'info' to see the logs in the terminal\n",
    "    # run this command in your terminal ~ tensorboard --logdir=./output_dir/runs\n",
    "    # and open 'http://localhost:6006/' to monitor the logs [loss over the training]\n",
    "\n",
    "    fp16=False                                   # ***** Mixed precision for faster training on A100; this won't work on CPU's\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4565a90c-fc40-48c1-b48d-457fc62d25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging in the Terminal\n",
    "class LogCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step {state.global_step}: {logs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076fbef-e719-41c7-a4af-fca399fbaae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143121a2-9d87-4484-bee1-37a768c6ae76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cba19c-9baa-4108-b63b-5b184e214c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e8f2f06-bd12-4a7c-b57f-1d113cae3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# # for training on the custom dataset\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "    \n",
    "#     train_dataset=tokenized_train_dataset,            # your test set\n",
    "#     # eval_dataset=tokenized_val_dataset,               # your validation set\n",
    "\n",
    "#     # train_dataset=tokenized_train_dataset.select(range(100)),    \n",
    "#     # eval_dataset=tokenized_val_dataset.select(range(100)),\n",
    "    \n",
    "#     # data_collator=data_collator, ----------------------------------------------------------------\n",
    "    \n",
    "#     callbacks=[LogCallback()]                   # *** to print the logs in the terminal\n",
    "# )\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# for training on the 5 different datasets mixed together and loaded from the tokenized parquet files\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "    \n",
    "#     train_dataset=tokenized_train_dataset,            # your test set\n",
    "#     eval_dataset=tokenized_val_dataset,               # your validation set\n",
    "\n",
    "#     # train_dataset=tokenized_train_dataset.select(range(100)),    \n",
    "#     # eval_dataset=tokenized_val_dataset.select(range(100)),\n",
    "    \n",
    "#     callbacks=[LogCallback()]                   # *** to print the logs in the terminal\n",
    "# )\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# for training on the 5 different datasets mixed by proportions specified for each of the batch\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= None,                     # overridden\n",
    "    # eval_dataset= tokenized_val_dataset,\n",
    "    callbacks=[LogCallback()]\n",
    ")\n",
    "\n",
    "# Override the train dataloader\n",
    "trainer.get_train_dataloader = lambda: ProportionalDataLoader(\n",
    "    datasets_dict,\n",
    "    proportions,\n",
    "    training_args.per_device_train_batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a367c20-fa53-4e15-b2b2-fa8a4da8b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20,000,000,000,000\n",
      "  Number of trainable parameters = 247,577,856\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1027' max='20000000000000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [          1027/20000000000000 08:04 < 2626373938:17:37, 2.12 it/s, Epoch 0.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>45.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>39.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>33.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>30.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>27.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>25.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>22.823800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>20.849400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>18.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>15.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>12.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>8.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>6.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.748100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>4.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.833700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.284800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.534700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.166800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: {'loss': 45.0949, 'grad_norm': 403.60491943359375, 'learning_rate': 9e-06, 'epoch': 1e-11}\n",
      "Step 20: {'loss': 39.2034, 'grad_norm': 969.8291625976562, 'learning_rate': 9.999999999995501e-06, 'epoch': 2e-11}\n",
      "Step 30: {'loss': 33.9411, 'grad_norm': 236.54627990722656, 'learning_rate': 9.9999999999905e-06, 'epoch': 3e-11}\n",
      "Step 40: {'loss': 30.4721, 'grad_norm': 280.5981750488281, 'learning_rate': 9.999999999985501e-06, 'epoch': 4e-11}\n",
      "Step 50: {'loss': 27.6411, 'grad_norm': 261.7852478027344, 'learning_rate': 9.9999999999805e-06, 'epoch': 5e-11}\n",
      "Step 60: {'loss': 25.1332, 'grad_norm': 165.44955444335938, 'learning_rate': 9.999999999975501e-06, 'epoch': 6e-11}\n",
      "Step 70: {'loss': 22.8238, 'grad_norm': 143.09695434570312, 'learning_rate': 9.9999999999705e-06, 'epoch': 7e-11}\n",
      "Step 80: {'loss': 20.8494, 'grad_norm': 150.2639923095703, 'learning_rate': 9.999999999965501e-06, 'epoch': 8e-11}\n",
      "Step 90: {'loss': 18.4301, 'grad_norm': 156.84312438964844, 'learning_rate': 9.999999999960502e-06, 'epoch': 9e-11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-100\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-100/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-100/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: {'loss': 15.1496, 'grad_norm': 240.167236328125, 'learning_rate': 9.999999999955501e-06, 'epoch': 1e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-100/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 110: {'loss': 12.2796, 'grad_norm': 74.86565399169922, 'learning_rate': 9.9999999999505e-06, 'epoch': 1.1e-10}\n",
      "Step 120: {'loss': 8.7983, 'grad_norm': 183.2331085205078, 'learning_rate': 9.999999999945501e-06, 'epoch': 1.2e-10}\n",
      "Step 130: {'loss': 6.2523, 'grad_norm': 38.465328216552734, 'learning_rate': 9.9999999999405e-06, 'epoch': 1.3e-10}\n",
      "Step 140: {'loss': 5.2658, 'grad_norm': 18.886783599853516, 'learning_rate': 9.999999999935501e-06, 'epoch': 1.4e-10}\n",
      "Step 150: {'loss': 4.7481, 'grad_norm': 13.156275749206543, 'learning_rate': 9.9999999999305e-06, 'epoch': 1.5e-10}\n",
      "Step 160: {'loss': 4.4888, 'grad_norm': 12.477185249328613, 'learning_rate': 9.999999999925501e-06, 'epoch': 1.6e-10}\n",
      "Step 170: {'loss': 4.3623, 'grad_norm': 15.485424995422363, 'learning_rate': 9.999999999920502e-06, 'epoch': 1.7e-10}\n",
      "Step 180: {'loss': 4.1832, 'grad_norm': 20.9342098236084, 'learning_rate': 9.9999999999155e-06, 'epoch': 1.8e-10}\n",
      "Step 190: {'loss': 4.0388, 'grad_norm': 21.470834732055664, 'learning_rate': 9.999999999910502e-06, 'epoch': 1.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-200\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-200/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-200/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: {'loss': 3.8337, 'grad_norm': 19.889909744262695, 'learning_rate': 9.9999999999055e-06, 'epoch': 2e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-200/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-100] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 210: {'loss': 3.657, 'grad_norm': 21.905519485473633, 'learning_rate': 9.9999999999005e-06, 'epoch': 2.1e-10}\n",
      "Step 220: {'loss': 3.4409, 'grad_norm': 23.796180725097656, 'learning_rate': 9.9999999998955e-06, 'epoch': 2.2e-10}\n",
      "Step 230: {'loss': 3.1994, 'grad_norm': 23.600038528442383, 'learning_rate': 9.9999999998905e-06, 'epoch': 2.3e-10}\n",
      "Step 240: {'loss': 3.0095, 'grad_norm': 22.47574234008789, 'learning_rate': 9.9999999998855e-06, 'epoch': 2.4e-10}\n",
      "Step 250: {'loss': 2.8378, 'grad_norm': 28.07978630065918, 'learning_rate': 9.999999999880501e-06, 'epoch': 2.5e-10}\n",
      "Step 260: {'loss': 2.7066, 'grad_norm': 22.416032791137695, 'learning_rate': 9.9999999998755e-06, 'epoch': 2.6e-10}\n",
      "Step 270: {'loss': 2.4256, 'grad_norm': 27.663860321044922, 'learning_rate': 9.999999999870501e-06, 'epoch': 2.7e-10}\n",
      "Step 280: {'loss': 2.2848, 'grad_norm': 22.29022979736328, 'learning_rate': 9.9999999998655e-06, 'epoch': 2.8e-10}\n",
      "Step 290: {'loss': 2.061, 'grad_norm': 23.251192092895508, 'learning_rate': 9.999999999860501e-06, 'epoch': 2.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-300\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-300/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-300/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: {'loss': 1.8604, 'grad_norm': 20.980995178222656, 'learning_rate': 9.9999999998555e-06, 'epoch': 3e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-300/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 310: {'loss': 1.6944, 'grad_norm': 18.09907341003418, 'learning_rate': 9.999999999850501e-06, 'epoch': 3.1e-10}\n",
      "Step 320: {'loss': 1.4756, 'grad_norm': 17.670705795288086, 'learning_rate': 9.9999999998455e-06, 'epoch': 3.2e-10}\n",
      "Step 330: {'loss': 1.3718, 'grad_norm': 17.549856185913086, 'learning_rate': 9.999999999840501e-06, 'epoch': 3.3e-10}\n",
      "Step 340: {'loss': 1.2147, 'grad_norm': 16.011398315429688, 'learning_rate': 9.999999999835502e-06, 'epoch': 3.4e-10}\n",
      "Step 350: {'loss': 1.0643, 'grad_norm': 12.636578559875488, 'learning_rate': 9.999999999830501e-06, 'epoch': 3.5e-10}\n",
      "Step 360: {'loss': 0.9452, 'grad_norm': 11.420974731445312, 'learning_rate': 9.9999999998255e-06, 'epoch': 3.6e-10}\n",
      "Step 370: {'loss': 0.8475, 'grad_norm': 11.727056503295898, 'learning_rate': 9.999999999820501e-06, 'epoch': 3.7e-10}\n",
      "Step 380: {'loss': 0.7415, 'grad_norm': 8.582765579223633, 'learning_rate': 9.9999999998155e-06, 'epoch': 3.8e-10}\n",
      "Step 390: {'loss': 0.6487, 'grad_norm': 8.17969799041748, 'learning_rate': 9.999999999810501e-06, 'epoch': 3.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-400\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-400/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-400/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: {'loss': 0.5938, 'grad_norm': 16.61419677734375, 'learning_rate': 9.9999999998055e-06, 'epoch': 4e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-400/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-300] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 410: {'loss': 0.5347, 'grad_norm': 6.523571014404297, 'learning_rate': 9.9999999998005e-06, 'epoch': 4.1e-10}\n",
      "Step 420: {'loss': 0.473, 'grad_norm': 5.449945449829102, 'learning_rate': 9.999999999795502e-06, 'epoch': 4.2e-10}\n",
      "Step 430: {'loss': 0.4156, 'grad_norm': 5.68875789642334, 'learning_rate': 9.9999999997905e-06, 'epoch': 4.3e-10}\n",
      "Step 440: {'loss': 0.392, 'grad_norm': 4.312360763549805, 'learning_rate': 9.999999999785502e-06, 'epoch': 4.4e-10}\n",
      "Step 450: {'loss': 0.353, 'grad_norm': 3.449847936630249, 'learning_rate': 9.9999999997805e-06, 'epoch': 4.5e-10}\n",
      "Step 460: {'loss': 0.3208, 'grad_norm': 2.99045991897583, 'learning_rate': 9.9999999997755e-06, 'epoch': 4.6e-10}\n",
      "Step 470: {'loss': 0.2947, 'grad_norm': 2.332852602005005, 'learning_rate': 9.9999999997705e-06, 'epoch': 4.7e-10}\n",
      "Step 480: {'loss': 0.2853, 'grad_norm': 3.0802416801452637, 'learning_rate': 9.999999999765501e-06, 'epoch': 4.8e-10}\n",
      "Step 490: {'loss': 0.2756, 'grad_norm': 1.5654637813568115, 'learning_rate': 9.9999999997605e-06, 'epoch': 4.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-500\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-500/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: {'loss': 0.2595, 'grad_norm': 1.8341302871704102, 'learning_rate': 9.999999999755501e-06, 'epoch': 5e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-500/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-400] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510: {'loss': 0.2209, 'grad_norm': 1.5108212232589722, 'learning_rate': 9.9999999997505e-06, 'epoch': 5.1e-10}\n",
      "Step 520: {'loss': 0.2256, 'grad_norm': 1.2204900979995728, 'learning_rate': 9.999999999745501e-06, 'epoch': 5.2e-10}\n",
      "Step 530: {'loss': 0.2035, 'grad_norm': 1.5909045934677124, 'learning_rate': 9.9999999997405e-06, 'epoch': 5.3e-10}\n",
      "Step 540: {'loss': 0.1921, 'grad_norm': 0.980563759803772, 'learning_rate': 9.999999999735501e-06, 'epoch': 5.4e-10}\n",
      "Step 550: {'loss': 0.1833, 'grad_norm': 0.9370376467704773, 'learning_rate': 9.9999999997305e-06, 'epoch': 5.5e-10}\n",
      "Step 560: {'loss': 0.1636, 'grad_norm': 0.7660162448883057, 'learning_rate': 9.999999999725501e-06, 'epoch': 5.6e-10}\n",
      "Step 570: {'loss': 0.1668, 'grad_norm': 0.9291651248931885, 'learning_rate': 9.9999999997205e-06, 'epoch': 5.7e-10}\n",
      "Step 580: {'loss': 0.1614, 'grad_norm': 0.6746436953544617, 'learning_rate': 9.999999999715501e-06, 'epoch': 5.8e-10}\n",
      "Step 590: {'loss': 0.157, 'grad_norm': 0.6578576564788818, 'learning_rate': 9.999999999710502e-06, 'epoch': 5.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-600\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-600/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-600/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600: {'loss': 0.1395, 'grad_norm': 0.6668218970298767, 'learning_rate': 9.999999999705501e-06, 'epoch': 6e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-600/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 610: {'loss': 0.1383, 'grad_norm': 0.6324518322944641, 'learning_rate': 9.9999999997005e-06, 'epoch': 6.1e-10}\n",
      "Step 620: {'loss': 0.1484, 'grad_norm': 0.6359052062034607, 'learning_rate': 9.999999999695501e-06, 'epoch': 6.2e-10}\n",
      "Step 630: {'loss': 0.1287, 'grad_norm': 0.5455908179283142, 'learning_rate': 9.9999999996905e-06, 'epoch': 6.3e-10}\n",
      "Step 640: {'loss': 0.1363, 'grad_norm': 0.46710920333862305, 'learning_rate': 9.999999999685501e-06, 'epoch': 6.4e-10}\n",
      "Step 650: {'loss': 0.1199, 'grad_norm': 0.47697532176971436, 'learning_rate': 9.9999999996805e-06, 'epoch': 6.5e-10}\n",
      "Step 660: {'loss': 0.1223, 'grad_norm': 0.5000445246696472, 'learning_rate': 9.9999999996755e-06, 'epoch': 6.6e-10}\n",
      "Step 670: {'loss': 0.1235, 'grad_norm': 0.46192026138305664, 'learning_rate': 9.999999999670502e-06, 'epoch': 6.7e-10}\n",
      "Step 680: {'loss': 0.1111, 'grad_norm': 0.42847028374671936, 'learning_rate': 9.9999999996655e-06, 'epoch': 6.8e-10}\n",
      "Step 690: {'loss': 0.1236, 'grad_norm': 0.41707828640937805, 'learning_rate': 9.999999999660502e-06, 'epoch': 6.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-700\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-700/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-700/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700: {'loss': 0.1192, 'grad_norm': 0.4100814759731293, 'learning_rate': 9.9999999996555e-06, 'epoch': 7e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-700/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-600] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 710: {'loss': 0.124, 'grad_norm': 0.48507651686668396, 'learning_rate': 9.9999999996505e-06, 'epoch': 7.1e-10}\n",
      "Step 720: {'loss': 0.1028, 'grad_norm': 0.4859665334224701, 'learning_rate': 9.9999999996455e-06, 'epoch': 7.2e-10}\n",
      "Step 730: {'loss': 0.114, 'grad_norm': 0.45177146792411804, 'learning_rate': 9.999999999640501e-06, 'epoch': 7.3e-10}\n",
      "Step 740: {'loss': 0.1075, 'grad_norm': 0.44294846057891846, 'learning_rate': 9.9999999996355e-06, 'epoch': 7.4e-10}\n",
      "Step 750: {'loss': 0.1067, 'grad_norm': 0.5272119045257568, 'learning_rate': 9.999999999630501e-06, 'epoch': 7.5e-10}\n",
      "Step 760: {'loss': 0.0991, 'grad_norm': 0.44872206449508667, 'learning_rate': 9.9999999996255e-06, 'epoch': 7.6e-10}\n",
      "Step 770: {'loss': 0.1121, 'grad_norm': 0.7051324844360352, 'learning_rate': 9.999999999620501e-06, 'epoch': 7.7e-10}\n",
      "Step 780: {'loss': 0.1033, 'grad_norm': 0.7304367423057556, 'learning_rate': 9.9999999996155e-06, 'epoch': 7.8e-10}\n",
      "Step 790: {'loss': 0.0962, 'grad_norm': 0.3796403706073761, 'learning_rate': 9.999999999610501e-06, 'epoch': 7.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-800\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-800/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-800/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800: {'loss': 0.0957, 'grad_norm': 0.40661147236824036, 'learning_rate': 9.9999999996055e-06, 'epoch': 8e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-800/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-700] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 810: {'loss': 0.0871, 'grad_norm': 0.43349704146385193, 'learning_rate': 9.999999999600501e-06, 'epoch': 8.1e-10}\n",
      "Step 820: {'loss': 0.0949, 'grad_norm': 0.419904887676239, 'learning_rate': 9.9999999995955e-06, 'epoch': 8.2e-10}\n",
      "Step 830: {'loss': 0.0984, 'grad_norm': 0.3862306773662567, 'learning_rate': 9.999999999590501e-06, 'epoch': 8.3e-10}\n",
      "Step 840: {'loss': 0.0929, 'grad_norm': 0.28797274827957153, 'learning_rate': 9.999999999585502e-06, 'epoch': 8.4e-10}\n",
      "Step 850: {'loss': 0.0938, 'grad_norm': 0.48715779185295105, 'learning_rate': 9.999999999580501e-06, 'epoch': 8.5e-10}\n",
      "Step 860: {'loss': 0.0984, 'grad_norm': 0.36179718375205994, 'learning_rate': 9.9999999995755e-06, 'epoch': 8.6e-10}\n",
      "Step 870: {'loss': 0.0873, 'grad_norm': 0.374920129776001, 'learning_rate': 9.999999999570501e-06, 'epoch': 8.7e-10}\n",
      "Step 880: {'loss': 0.0872, 'grad_norm': 0.35798391699790955, 'learning_rate': 9.9999999995655e-06, 'epoch': 8.8e-10}\n",
      "Step 890: {'loss': 0.0832, 'grad_norm': 0.27132055163383484, 'learning_rate': 9.9999999995605e-06, 'epoch': 8.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-900\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-900/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-900/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900: {'loss': 0.0859, 'grad_norm': 0.27623653411865234, 'learning_rate': 9.999999999555502e-06, 'epoch': 9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-900/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-800] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 910: {'loss': 0.086, 'grad_norm': 0.3675633370876312, 'learning_rate': 9.9999999995505e-06, 'epoch': 9.1e-10}\n",
      "Step 920: {'loss': 0.0872, 'grad_norm': 0.4046382009983063, 'learning_rate': 9.999999999545502e-06, 'epoch': 9.2e-10}\n",
      "Step 930: {'loss': 0.0769, 'grad_norm': 0.2977201044559479, 'learning_rate': 9.9999999995405e-06, 'epoch': 9.3e-10}\n",
      "Step 940: {'loss': 0.0813, 'grad_norm': 0.39227205514907837, 'learning_rate': 9.999999999535502e-06, 'epoch': 9.4e-10}\n",
      "Step 950: {'loss': 0.0724, 'grad_norm': 0.3691612780094147, 'learning_rate': 9.9999999995305e-06, 'epoch': 9.5e-10}\n",
      "Step 960: {'loss': 0.0822, 'grad_norm': 0.3463331460952759, 'learning_rate': 9.9999999995255e-06, 'epoch': 9.6e-10}\n",
      "Step 970: {'loss': 0.078, 'grad_norm': 0.34928029775619507, 'learning_rate': 9.9999999995205e-06, 'epoch': 9.7e-10}\n",
      "Step 980: {'loss': 0.0723, 'grad_norm': 0.39660942554473877, 'learning_rate': 9.999999999515501e-06, 'epoch': 9.8e-10}\n",
      "Step 990: {'loss': 0.0796, 'grad_norm': 0.35483866930007935, 'learning_rate': 9.9999999995105e-06, 'epoch': 9.9e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5flanbase_Question_Generation/checkpoint-1000\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-1000/config.json\n",
      "Configuration saved in ./T5flanbase_Question_Generation/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: {'loss': 0.0734, 'grad_norm': 0.2984829246997833, 'learning_rate': 9.999999999505501e-06, 'epoch': 1e-09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5flanbase_Question_Generation/checkpoint-1000/model.safetensors\n",
      "Deleting older checkpoint [T5flanbase_Question_Generation/checkpoint-900] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1010: {'loss': 0.0747, 'grad_norm': 0.5894613265991211, 'learning_rate': 9.9999999995005e-06, 'epoch': 1.01e-09}\n",
      "Step 1020: {'loss': 0.0742, 'grad_norm': 0.4583534300327301, 'learning_rate': 9.999999999495501e-06, 'epoch': 1.02e-09}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training the Model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(save_path)                           \n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2565\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(save_path)                           \n",
    "\n",
    "tokenizer.save_pretrained(save_path)                    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166932f-d6e2-428b-a237-897e3c86d188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a5925-4845-4861-bebb-011647f358d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f478ac-861b-4415-9ca5-d671816b91be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bdc5c-d925-450f-8329-24a161a460a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d61cca-3e83-42f5-86a7-62e1087c3519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7840e092-7536-4fae-b74d-85ab8e2a435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Model config T5Config {\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory T5flanbase_Question_Generation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4251\u001b[0m     gguf_file\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4254\u001b[0m ):\n\u001b[1;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m     )\n\u001b[0;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:952\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m         )\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m    958\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory T5flanbase_Question_Generation."
     ]
    }
   ],
   "source": [
    "\n",
    "# the finetuned model name\n",
    "new_model = \"T5flanbase_Question_Generation\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(new_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5a2ceb7-4950-4e76-b9cb-94c9d6ae66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the finetuned question generation model with a sample context\n",
    "\n",
    "\n",
    "def get_question(tag, difficulty, context, answer=\"\", num_questions=3, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate questions using the fine-tuned T5 model.\n",
    "    \n",
    "    Parameters:\n",
    "    - tag: Type of question (e.g., \"short answer\", \"multiple choice question\", \"true or false question\")\n",
    "    - difficulty: \"easy\", \"medium\", \"hard\"\n",
    "    - context: Supporting context or passage\n",
    "    - answer: Optional — if you want targeted question generation\n",
    "    - num_questions: Number of diverse questions to generate\n",
    "    - max_length: Max token length of generated output\n",
    "    \n",
    "    Returns:\n",
    "    - List of generated questions as strings\n",
    "    \"\"\"\n",
    "    # Format input text based on whether answer is provided\n",
    "    answer_part = f\"[{answer}]\" if answer else \"\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{answer_part} {context}\"\n",
    "\n",
    "    # Tokenize\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    # Generate questions\n",
    "    output = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_questions,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    # Decode generated questions\n",
    "    return [tokenizer.decode(out, skip_special_tokens=True) for out in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0cdca68-49a0-4884-8c1b-a724d6fc304e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# short answer question\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_question\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshort answer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCadmium chloride is a hygroscopic solid that is highly soluble in water and slightly soluble in alcohol. Ethanol, also called alcohol, ethyl alcohol, and drinking alcohol, is a compound and simple alcohol with the chemical formula C2H5OH.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[28], line 27\u001b[0m, in \u001b[0;36mget_question\u001b[0;34m(tag, difficulty, context, answer, num_questions, max_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m features \u001b[38;5;241m=\u001b[39m tokenizer([input_text], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Generate questions\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Decode generated questions\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m output]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:2280\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   2279\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 2280\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2284\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   2285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:778\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    776\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    777\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 778\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1009\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1009\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# short answer question\n",
    "print(get_question(\n",
    "    tag=\"short answer\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"Cadmium chloride is a hygroscopic solid that is highly soluble in water and slightly soluble in alcohol. Ethanol, also called alcohol, ethyl alcohol, and drinking alcohol, is a compound and simple alcohol with the chemical formula C2H5OH.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409aab2c-fa28-47c6-82d9-041c9ed85b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true or false question\n",
    "print(get_question(\n",
    "    tag=\"true or false question\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"Ethanol, also known as drinking alcohol, is a clear, colorless liquid that is flammable and is produced by the fermentation of sugars by yeast. It has the chemical formula C2H5OH and is used both recreationally and industrially.\"\n",
    "))\n",
    "\n",
    "print(get_question(\n",
    "    tag=\"true or false question\",\n",
    "    difficulty=\"easy\",\n",
    "    context=\"Ethanol, also known as drinking alcohol.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46873527-532a-4961-94f9-f0d056eaab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple choice question\n",
    "print(get_question(\n",
    "    tag=\"multiple choice question\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"Ethanol is used as a recreational beverage, as a solvent, and as a fuel additive. It is a volatile, flammable, colorless liquid with a slight characteristic odor, and its chemical formula is C2H5OH.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00300d22-e260-42eb-a585-2a7cd1bbd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short answer question\n",
    "print(get_question(\n",
    "    tag=\"short answer\",\n",
    "    difficulty=\"medium\",\n",
    "    context=\"A computer is an electronic device that takes in data (input), processes it using hardware and software, stores it, and then produces information (output). At its core, it manipulates binary code (1s and 0s) through transistors and logic gates, performing complex tasks incredibly fast, making it efficient for everything from simple calculations to running complex applications\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53e1c8-d2bb-46ac-93b1-1f6eeda0e03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
