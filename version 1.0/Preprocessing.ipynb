{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323041f9-0441-4993-a2d8-5a47c1c3e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the directory where we want to store the models\n",
    "import os\n",
    "\n",
    "print(\"Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - \" + os.getcwd())\n",
    "os.environ['HF_HOME'] = \"/home/aalla4\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/home/aalla4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d5255-749f-4d6c-97f3-7797d1e1f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "\n",
    "# getting the CPU and GPU count\n",
    "\n",
    "print(\"Total number of logical cores = \" + str(os.cpu_count()))  # This shows logical cores not the physical cores\n",
    "LOGICAL_CORES = os.cpu_count()\n",
    "USABLE_CPU_CORES = LOGICAL_CORES - 2    # YOU CAN CHANGE THIS ACCORDING TO THE CPU AVAILABILITIES\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb30b8-0495-4f0c-8fb9-587655dd6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer \n",
    "\n",
    "model_name = \"./T5base_Question_Generation_v6\" \n",
    "\n",
    "save_path = \"./T5base_Question_Generation_v7\"\n",
    "tokenized_trainset_save_path =  \"./dataset/custom_dataset/tokenized_trainset.parquet\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Max positional embeddings supported by model - {model_name}: \", model.config.n_positions)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d20b39-9226-48ac-b913-c00a8c621545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# Function to load the data from a JSON file\n",
    "import json\n",
    "\n",
    "filepath_descriptive = \"./dataset/custom_dataset/descriptive.json\" \n",
    "filepath_mcq = \"./dataset/custom_dataset/mcq.json\" \n",
    "filepath_tf = \"./dataset/custom_dataset/true_false.json\" \n",
    "\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)                    # This should be a list of dicts\n",
    "\n",
    "        \n",
    "descriptive_data = load_json_file(filepath_descriptive)\n",
    "mcq_data = load_json_file(filepath_mcq)\n",
    "tf_data = load_json_file(filepath_tf)\n",
    "\n",
    "# Convert list of dicts to Hugging Face Dataset\n",
    "dataset_descriptive = Dataset.from_list(descriptive_data)\n",
    "dataset_mcq = Dataset.from_list(mcq_data)\n",
    "dataset_tf = Dataset.from_list(tf_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23454399-7792-48e2-a1f3-b6f38d8f147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing function to tokenize the input and target text\n",
    "def preprocess_descriptive(example):\n",
    "    '''<extra_id_97>short answer question <extra_id_98>easy <extra_id_99>Drinking enough water each day helps regulate body temperature, keep joints lubricated, prevent infections, and keep organs functioning properly. Proper hydration also improves sleep quality, cognition, and mood. \n",
    "       List two ways drinking water benefits the human body.'''\n",
    "    tag = example[\"tag\"]\n",
    "    difficulty = example[\"difficulty\"]\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = question\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_mcq(example):\n",
    "    '''there has been a mistake in the dataset so we have considered all the mcq to be of medium difficulty'''\n",
    "    '''<extra_id_97>multiple choice question <extra_id_98>medium <extra_id_99>Rainforests are essential to Earth’s ecosystem. They produce oxygen, absorb carbon dioxide, and help regulate the global climate. Rainforests are also home to more than half of the world’s plant and animal species. Despite their importance, they are being destroyed at an alarming rate due to logging, agriculture, and mining. When rainforests are cleared, biodiversity is lost, and carbon is released into the atmosphere, contributing to global warming. Indigenous people who depend on these forests are also displaced. Preserving rainforests is vital for maintaining environmental balance and protecting wildlife. \n",
    "       Which of the following is a consequence of rainforest destruction? [C. Global warming] (A. Improved biodiversity; B. Carbon absorption; C. Global warming; D. Increased rainfall)'''\n",
    "    tag = example[\"difficulty\"]\n",
    "    difficulty = \"medium\"\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    options = example[\"options\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    # Prepare formatted options\n",
    "    option_labels = ['A', 'B', 'C', 'D']\n",
    "    formatted_options = [f\"{label}. {opt}\" for label, opt in zip(option_labels, options)]\n",
    "    correct_index = options.index(answer)\n",
    "    correct_option = f\"{option_labels[correct_index]}. {answer}\"\n",
    "\n",
    "    # Prepare raw input and label strings\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = f\"{question} [{correct_option}] ({'; '.join(formatted_options)})\"\n",
    "\n",
    "    # Tokenize input and target\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Add labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_tf(example):\n",
    "    '''<extra_id_97>true or false question <extra_id_98>easy <extra_id_99>Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water. The process typically occurs in the chloroplasts of plant cells and releases oxygen as a byproduct. Chlorophyll, the green pigment in plants, plays a crucial role in capturing light energy. This energy is then used to convert water and carbon dioxide into glucose, which serves as the plant’s food source. \n",
    "        Photosynthesis releases oxygen as a byproduct. [true]'''\n",
    "    tag = example[\"tag\"]\n",
    "    difficulty = example[\"difficulty\"]\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"].lower()  # ensure it's \"true\" or \"false\"\n",
    "\n",
    "    # Format the input and target\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "    target_text = f\"{question} [{answer}]\"\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d86dd7-d5d6-4a7e-a247-0e841bef1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_proc = 1\n",
    "\n",
    "processed_descriptive_dataset = dataset_descriptive.map(preprocess_descriptive, batched=False, num_proc=num_proc, remove_columns=dataset_descriptive.column_names)\n",
    "processed_mcq_dataset = dataset_mcq.map(preprocess_mcq, batched=False, num_proc=num_proc, remove_columns=dataset_mcq.column_names)\n",
    "processed_tf_dataset = dataset_tf.map(preprocess_tf, batched=False, num_proc=num_proc, remove_columns=dataset_tf.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bafb49-b45e-4f27-b6b6-0468d42d52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lengths = [len(example['labels']) for example in processed_descriptive_dataset]\n",
    "print(f\"Min label length: {min(label_lengths)}, Max label length: {max(label_lengths)}\")\n",
    "\n",
    "# Count how many have length exactly 250\n",
    "count_250 = sum(1 for length in label_lengths if length == 250)\n",
    "print(f\"Number of examples with label length 250: {count_250}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede32f9f-e4bf-4a46-bbb6-2a227af5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processed_descriptive_dataset)\n",
    "# print(processed_mcq_dataset)\n",
    "# print(processed_tf_dataset)\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#     num_rows: 5\n",
    "# })\n",
    "\n",
    "tokenized_train_dataset = concatenate_datasets([\n",
    "    processed_descriptive_dataset,\n",
    "    processed_mcq_dataset,\n",
    "    processed_tf_dataset\n",
    "]).shuffle(seed=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e62048-0b49-4dcc-b774-bd518df80449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_trainset_save_path = \"./dataset/custom_dataset/tokenized_trainset.parquet\"\n",
    "\n",
    "tokenized_train_dataset.to_parquet(tokenized_trainset_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cc1ed-b703-4564-839b-d684f850fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fba5a-635e-44a1-9b79-ab1b4c1825d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9816d-738f-44b1-847c-9e3445a24f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da14decf-5837-4bdc-93c4-462540df0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - /home/aalla4\n"
     ]
    }
   ],
   "source": [
    "# setting up the directory where we want to store the models\n",
    "import os\n",
    "\n",
    "print(\"Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - \" + os.getcwd())\n",
    "os.environ['HF_HOME'] = \"/home/aalla4\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/home/aalla4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68c6565-fe74-45f3-851b-e3ad3ba47721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a7535a-9f85-463a-b0d4-3f0ef626ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of logical cores = 48\n",
      "CUDA available:  True\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# getting the CPU and GPU count\n",
    "\n",
    "print(\"Total number of logical cores = \" + str(os.cpu_count()))  # This shows logical cores not the physical cores\n",
    "LOGICAL_CORES = os.cpu_count()\n",
    "USABLE_CPU_CORES = LOGICAL_CORES - 1    # YOU CAN CHANGE THIS ACCORDING TO THE CPU AVAILABILITIES\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4899ce80-3619-4bc8-ad98-f5ffcf7fdb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max positional embeddings supported by model - t5-base:  512\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer \n",
    "\n",
    "model_name = \"t5-base\"\n",
    "\n",
    "save_path = \"./T5base_Question_Generation_v0_custom_dataset\"\n",
    "tokenized_trainset_path = \"./dataset/custom_dataset/tokenized_trainset.parquet\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Max positional embeddings supported by model - {model_name}: \", model.config.n_positions)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a85427-2b7c-4d54-b165-8c7a520720b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenized Dataset if Preprocessing is already done\n",
    "\n",
    "# Load from Parquet\n",
    "tokenized_train_dataset = Dataset.from_parquet(tokenized_trainset_path)\n",
    "\n",
    "# tokenized_train_dataset = Dataset.from_parquet(tokenized_trainset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5df8d1-39d2-4dc3-b559-0d08e3bdc0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 328\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#     num_rows: 5\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6bbd7db-513d-4588-a40d-928e40431788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(tokenized_train_dataset)  # Should be [batch_size, 512] or similar\n",
    "\n",
    "# print(len(tokenized_train_dataset[0]['input_ids']))\n",
    "# print(len(tokenized_train_dataset[0]['labels']))\n",
    "\n",
    "\n",
    "# label_lengths = [len(example['labels']) for example in tokenized_train_dataset]\n",
    "# print(f\"Min label length: {min(label_lengths)}, Max label length: {max(label_lengths)}\")\n",
    "\n",
    "# # Count how many have length exactly 250\n",
    "# count_250 = sum(1 for length in label_lengths if length == 250)\n",
    "# print(f\"Number of examples with label length 250: {count_250}\")\n",
    "\n",
    "# Check the length of a few tokenized examples\n",
    "# for i in range(5):\n",
    "#     print(len(tokenized_train_dataset[i]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269089c7-eef8-4b84-ae94-b170e6c5d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the current device:  cuda\n",
      "Device updated to: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Defning the Training args \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"This is the current device: \", device)\n",
    "print(\"Device updated to:\", model.device)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,              # directory where the training logs, checkpoints, and evaluation results (like metrics) are saved during the training process   \n",
    "    \n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=7,                         \n",
    "    weight_decay=1e-3,                          \n",
    "\n",
    "    # generally Batch size = per_device_train_batch_size * per_device_train_batch_size\n",
    "    per_device_train_batch_size=3,                       # ****** if using GPU's = 16; if using CPU's = 1 or 2\n",
    "    # gradient_accumulation_steps=2,                     # Increase this if you need to simulate larger batch sizes, without running into 'Out or Memory' errors when memory is limited\n",
    "    # per_device_eval_batch_size=8,\n",
    "\n",
    "    # this is not useful for CPU based training as hugging face trainer handles multi-core utilization automatically based on the system configuration\n",
    "    dataloader_num_workers= USABLE_CPU_CORES,          # ****** for optimal use of CPU and not wasting GPU time [ this helps in loading the next batch of data into the VRAM ]\n",
    "\n",
    "    # Print validation loss every epoch\n",
    "    # eval_strategy=\"epoch\",            \n",
    "\n",
    "    # Print and logs the training loss of the training data\n",
    "    logging_strategy=\"steps\",   \n",
    "    logging_steps=5,                                   # ****** if using GPU = 100; if using CPU = 1 or 2 \n",
    "\n",
    "    # saves model at the end of every epoch\n",
    "    save_strategy=\"epoch\",            \n",
    "    # save_total_limit=2,\n",
    "    save_total_limit=1,\n",
    "\n",
    "    # report_to=\"none\",  # Disable default logging\n",
    "    \n",
    "    logging_dir= save_path + \"/logs\",           # save logs to a directory\n",
    "    report_to=\"tensorboard\",                    # Reports to TensorBoard\n",
    "    log_level='info',                           # Set logging level to 'info' to see the logs in the terminal\n",
    "    # run this command in your terminal ~ tensorboard --logdir=./output_dir/runs\n",
    "    # and open 'http://localhost:6006/' to monitor the logs [loss over the training]\n",
    "\n",
    "    fp16=False                                   # ***** Mixed precision for faster training on A100; this won't work on CPU's\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3e498b-497f-4f8d-b84d-53bf32ca135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging in the Terminal\n",
    "class LogCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step {state.global_step}: {logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a621fac-f5e1-43ff-ba21-739c57c16408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=tokenized_train_dataset,            # your test set\n",
    "    # eval_dataset=tokenized_val_dataset,               # your validation set\n",
    "\n",
    "    # train_dataset=tokenized_train_dataset.select(range(100)),    \n",
    "    # eval_dataset=tokenized_val_dataset.select(range(100)),\n",
    "    \n",
    "    callbacks=[LogCallback()]                   # *** to print the logs in the terminal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a91016-00d3-451a-bad9-b3f09bb54062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 328\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 770\n",
      "  Number of trainable parameters = 222,903,552\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 03:36, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>19.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>19.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>16.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>15.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>13.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>12.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>10.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>8.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>6.802100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>5.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.365100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.276500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>0.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.205800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: {'loss': 19.6844, 'grad_norm': 123.0082778930664, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.045454545454545456}\n",
      "Step 10: {'loss': 19.2324, 'grad_norm': 120.02975463867188, 'learning_rate': 9e-06, 'epoch': 0.09090909090909091}\n",
      "Step 15: {'loss': 16.7283, 'grad_norm': 97.46022033691406, 'learning_rate': 9.947368421052632e-06, 'epoch': 0.13636363636363635}\n",
      "Step 20: {'loss': 15.2924, 'grad_norm': 206.221435546875, 'learning_rate': 9.881578947368422e-06, 'epoch': 0.18181818181818182}\n",
      "Step 25: {'loss': 13.6819, 'grad_norm': 67.22552490234375, 'learning_rate': 9.815789473684212e-06, 'epoch': 0.22727272727272727}\n",
      "Step 30: {'loss': 12.2475, 'grad_norm': 70.75728607177734, 'learning_rate': 9.75e-06, 'epoch': 0.2727272727272727}\n",
      "Step 35: {'loss': 10.8149, 'grad_norm': 50.66968536376953, 'learning_rate': 9.68421052631579e-06, 'epoch': 0.3181818181818182}\n",
      "Step 40: {'loss': 9.1736, 'grad_norm': 55.1870231628418, 'learning_rate': 9.61842105263158e-06, 'epoch': 0.36363636363636365}\n",
      "Step 45: {'loss': 8.4071, 'grad_norm': 77.59667205810547, 'learning_rate': 9.552631578947369e-06, 'epoch': 0.4090909090909091}\n",
      "Step 50: {'loss': 7.4858, 'grad_norm': 57.53448486328125, 'learning_rate': 9.486842105263158e-06, 'epoch': 0.45454545454545453}\n",
      "Step 55: {'loss': 6.8021, 'grad_norm': 30.872764587402344, 'learning_rate': 9.421052631578949e-06, 'epoch': 0.5}\n",
      "Step 60: {'loss': 5.5577, 'grad_norm': 36.58281326293945, 'learning_rate': 9.355263157894738e-06, 'epoch': 0.5454545454545454}\n",
      "Step 65: {'loss': 5.1489, 'grad_norm': 47.47823715209961, 'learning_rate': 9.289473684210527e-06, 'epoch': 0.5909090909090909}\n",
      "Step 70: {'loss': 3.7588, 'grad_norm': 40.12375259399414, 'learning_rate': 9.223684210526316e-06, 'epoch': 0.6363636363636364}\n",
      "Step 75: {'loss': 2.835, 'grad_norm': 26.26176643371582, 'learning_rate': 9.157894736842105e-06, 'epoch': 0.6818181818181818}\n",
      "Step 80: {'loss': 2.2812, 'grad_norm': 21.61469841003418, 'learning_rate': 9.092105263157896e-06, 'epoch': 0.7272727272727273}\n",
      "Step 85: {'loss': 1.4055, 'grad_norm': 18.731115341186523, 'learning_rate': 9.026315789473685e-06, 'epoch': 0.7727272727272727}\n",
      "Step 90: {'loss': 1.4488, 'grad_norm': 17.319597244262695, 'learning_rate': 8.960526315789474e-06, 'epoch': 0.8181818181818182}\n",
      "Step 95: {'loss': 0.9664, 'grad_norm': 10.280051231384277, 'learning_rate': 8.894736842105264e-06, 'epoch': 0.8636363636363636}\n",
      "Step 100: {'loss': 0.7865, 'grad_norm': 7.6268696784973145, 'learning_rate': 8.828947368421054e-06, 'epoch': 0.9090909090909091}\n",
      "Step 105: {'loss': 0.6131, 'grad_norm': 3.9395992755889893, 'learning_rate': 8.763157894736842e-06, 'epoch': 0.9545454545454546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset/checkpoint-110\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-110/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-110/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 110: {'loss': 0.5892, 'grad_norm': 3.8145999908447266, 'learning_rate': 8.697368421052633e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-110/model.safetensors\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 115: {'loss': 0.4605, 'grad_norm': 2.281625747680664, 'learning_rate': 8.631578947368422e-06, 'epoch': 1.0454545454545454}\n",
      "Step 120: {'loss': 0.5103, 'grad_norm': 1.49209725856781, 'learning_rate': 8.565789473684213e-06, 'epoch': 1.0909090909090908}\n",
      "Step 125: {'loss': 0.4734, 'grad_norm': 2.4539108276367188, 'learning_rate': 8.5e-06, 'epoch': 1.1363636363636362}\n",
      "Step 130: {'loss': 0.4409, 'grad_norm': 1.1102465391159058, 'learning_rate': 8.43421052631579e-06, 'epoch': 1.1818181818181819}\n",
      "Step 135: {'loss': 0.493, 'grad_norm': 1.6525719165802002, 'learning_rate': 8.36842105263158e-06, 'epoch': 1.2272727272727273}\n",
      "Step 140: {'loss': 0.44, 'grad_norm': 1.2360838651657104, 'learning_rate': 8.302631578947369e-06, 'epoch': 1.2727272727272727}\n",
      "Step 145: {'loss': 0.3651, 'grad_norm': 1.86359441280365, 'learning_rate': 8.236842105263158e-06, 'epoch': 1.3181818181818181}\n",
      "Step 150: {'loss': 0.3832, 'grad_norm': 1.121274471282959, 'learning_rate': 8.171052631578949e-06, 'epoch': 1.3636363636363638}\n",
      "Step 155: {'loss': 0.4808, 'grad_norm': 1.87747323513031, 'learning_rate': 8.105263157894736e-06, 'epoch': 1.4090909090909092}\n",
      "Step 160: {'loss': 0.3517, 'grad_norm': 1.6193760633468628, 'learning_rate': 8.039473684210527e-06, 'epoch': 1.4545454545454546}\n",
      "Step 165: {'loss': 0.3784, 'grad_norm': 1.0593656301498413, 'learning_rate': 7.973684210526316e-06, 'epoch': 1.5}\n",
      "Step 170: {'loss': 0.3603, 'grad_norm': 0.8483625650405884, 'learning_rate': 7.907894736842105e-06, 'epoch': 1.5454545454545454}\n",
      "Step 175: {'loss': 0.3597, 'grad_norm': 0.9372991919517517, 'learning_rate': 7.842105263157895e-06, 'epoch': 1.5909090909090908}\n",
      "Step 180: {'loss': 0.3047, 'grad_norm': 0.8012388348579407, 'learning_rate': 7.776315789473685e-06, 'epoch': 1.6363636363636362}\n",
      "Step 185: {'loss': 0.3994, 'grad_norm': 0.9538162350654602, 'learning_rate': 7.710526315789474e-06, 'epoch': 1.6818181818181817}\n",
      "Step 190: {'loss': 0.3033, 'grad_norm': 0.746371865272522, 'learning_rate': 7.644736842105264e-06, 'epoch': 1.7272727272727273}\n",
      "Step 195: {'loss': 0.3152, 'grad_norm': 0.8798379898071289, 'learning_rate': 7.578947368421054e-06, 'epoch': 1.7727272727272727}\n",
      "Step 200: {'loss': 0.3387, 'grad_norm': 3.0064170360565186, 'learning_rate': 7.513157894736842e-06, 'epoch': 1.8181818181818183}\n",
      "Step 205: {'loss': 0.2906, 'grad_norm': 0.8416454792022705, 'learning_rate': 7.447368421052632e-06, 'epoch': 1.8636363636363638}\n",
      "Step 210: {'loss': 0.3514, 'grad_norm': 0.924263596534729, 'learning_rate': 7.381578947368422e-06, 'epoch': 1.9090909090909092}\n",
      "Step 215: {'loss': 0.335, 'grad_norm': 1.0941787958145142, 'learning_rate': 7.315789473684212e-06, 'epoch': 1.9545454545454546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset/checkpoint-220\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-220/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-220/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 220: {'loss': 0.2993, 'grad_norm': 1.1355339288711548, 'learning_rate': 7.25e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-220/model.safetensors\n",
      "Deleting older checkpoint [T5base_Question_Generation_v0_custom_dataset/checkpoint-110] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 225: {'loss': 0.3539, 'grad_norm': 1.1638354063034058, 'learning_rate': 7.18421052631579e-06, 'epoch': 2.0454545454545454}\n",
      "Step 230: {'loss': 0.2982, 'grad_norm': 0.702418327331543, 'learning_rate': 7.11842105263158e-06, 'epoch': 2.090909090909091}\n",
      "Step 235: {'loss': 0.3494, 'grad_norm': 0.7854982018470764, 'learning_rate': 7.052631578947369e-06, 'epoch': 2.1363636363636362}\n",
      "Step 240: {'loss': 0.3152, 'grad_norm': 0.9248342514038086, 'learning_rate': 6.986842105263158e-06, 'epoch': 2.1818181818181817}\n",
      "Step 245: {'loss': 0.281, 'grad_norm': 0.6607226729393005, 'learning_rate': 6.921052631578948e-06, 'epoch': 2.227272727272727}\n",
      "Step 250: {'loss': 0.267, 'grad_norm': 0.8154204487800598, 'learning_rate': 6.855263157894737e-06, 'epoch': 2.2727272727272725}\n",
      "Step 255: {'loss': 0.2863, 'grad_norm': 0.7409448027610779, 'learning_rate': 6.789473684210527e-06, 'epoch': 2.3181818181818183}\n",
      "Step 260: {'loss': 0.266, 'grad_norm': 0.7227839827537537, 'learning_rate': 6.723684210526316e-06, 'epoch': 2.3636363636363638}\n",
      "Step 265: {'loss': 0.3276, 'grad_norm': 1.9187606573104858, 'learning_rate': 6.6578947368421055e-06, 'epoch': 2.409090909090909}\n",
      "Step 270: {'loss': 0.3173, 'grad_norm': 0.7221177816390991, 'learning_rate': 6.5921052631578955e-06, 'epoch': 2.4545454545454546}\n",
      "Step 275: {'loss': 0.2722, 'grad_norm': 0.9330942630767822, 'learning_rate': 6.526315789473685e-06, 'epoch': 2.5}\n",
      "Step 280: {'loss': 0.2757, 'grad_norm': 0.7484189867973328, 'learning_rate': 6.460526315789474e-06, 'epoch': 2.5454545454545454}\n",
      "Step 285: {'loss': 0.2864, 'grad_norm': 1.351959228515625, 'learning_rate': 6.394736842105264e-06, 'epoch': 2.590909090909091}\n",
      "Step 290: {'loss': 0.2538, 'grad_norm': 9.265350341796875, 'learning_rate': 6.328947368421054e-06, 'epoch': 2.6363636363636362}\n",
      "Step 295: {'loss': 0.3215, 'grad_norm': 0.8205291032791138, 'learning_rate': 6.263157894736842e-06, 'epoch': 2.6818181818181817}\n",
      "Step 300: {'loss': 0.2352, 'grad_norm': 0.9804475903511047, 'learning_rate': 6.197368421052632e-06, 'epoch': 2.7272727272727275}\n",
      "Step 305: {'loss': 0.2765, 'grad_norm': 1.7008705139160156, 'learning_rate': 6.131578947368422e-06, 'epoch': 2.7727272727272725}\n",
      "Step 310: {'loss': 0.2805, 'grad_norm': 0.8851985335350037, 'learning_rate': 6.065789473684212e-06, 'epoch': 2.8181818181818183}\n",
      "Step 315: {'loss': 0.3185, 'grad_norm': 1.0642179250717163, 'learning_rate': 6e-06, 'epoch': 2.8636363636363638}\n",
      "Step 320: {'loss': 0.2979, 'grad_norm': 0.7776870727539062, 'learning_rate': 5.93421052631579e-06, 'epoch': 2.909090909090909}\n",
      "Step 325: {'loss': 0.2626, 'grad_norm': 0.9176682829856873, 'learning_rate': 5.86842105263158e-06, 'epoch': 2.9545454545454546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset/checkpoint-330\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-330/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-330/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 330: {'loss': 0.2527, 'grad_norm': 1.2695918083190918, 'learning_rate': 5.802631578947368e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-330/model.safetensors\n",
      "Deleting older checkpoint [T5base_Question_Generation_v0_custom_dataset/checkpoint-220] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 335: {'loss': 0.2872, 'grad_norm': 0.7799136638641357, 'learning_rate': 5.736842105263158e-06, 'epoch': 3.0454545454545454}\n",
      "Step 340: {'loss': 0.2573, 'grad_norm': 0.9460180997848511, 'learning_rate': 5.671052631578948e-06, 'epoch': 3.090909090909091}\n",
      "Step 345: {'loss': 0.2619, 'grad_norm': 0.781097948551178, 'learning_rate': 5.605263157894737e-06, 'epoch': 3.1363636363636362}\n",
      "Step 350: {'loss': 0.2288, 'grad_norm': 0.8546037077903748, 'learning_rate': 5.5394736842105266e-06, 'epoch': 3.1818181818181817}\n",
      "Step 355: {'loss': 0.2706, 'grad_norm': 0.731988251209259, 'learning_rate': 5.4736842105263165e-06, 'epoch': 3.227272727272727}\n",
      "Step 360: {'loss': 0.2253, 'grad_norm': 0.6366394758224487, 'learning_rate': 5.407894736842106e-06, 'epoch': 3.2727272727272725}\n",
      "Step 365: {'loss': 0.3068, 'grad_norm': 0.7625462412834167, 'learning_rate': 5.342105263157895e-06, 'epoch': 3.3181818181818183}\n",
      "Step 370: {'loss': 0.2598, 'grad_norm': 0.6551209688186646, 'learning_rate': 5.276315789473685e-06, 'epoch': 3.3636363636363638}\n",
      "Step 375: {'loss': 0.248, 'grad_norm': 0.5885505676269531, 'learning_rate': 5.210526315789474e-06, 'epoch': 3.409090909090909}\n",
      "Step 380: {'loss': 0.2685, 'grad_norm': 1.2480202913284302, 'learning_rate': 5.144736842105264e-06, 'epoch': 3.4545454545454546}\n",
      "Step 385: {'loss': 0.2814, 'grad_norm': 0.7467803359031677, 'learning_rate': 5.078947368421053e-06, 'epoch': 3.5}\n",
      "Step 390: {'loss': 0.2939, 'grad_norm': 0.8181697130203247, 'learning_rate': 5.013157894736842e-06, 'epoch': 3.5454545454545454}\n",
      "Step 395: {'loss': 0.2523, 'grad_norm': 1.4519582986831665, 'learning_rate': 4.947368421052632e-06, 'epoch': 3.590909090909091}\n",
      "Step 400: {'loss': 0.2589, 'grad_norm': 1.290526270866394, 'learning_rate': 4.881578947368422e-06, 'epoch': 3.6363636363636362}\n",
      "Step 405: {'loss': 0.2654, 'grad_norm': 0.7928181886672974, 'learning_rate': 4.815789473684211e-06, 'epoch': 3.6818181818181817}\n",
      "Step 410: {'loss': 0.2527, 'grad_norm': 1.6419265270233154, 'learning_rate': 4.75e-06, 'epoch': 3.7272727272727275}\n",
      "Step 415: {'loss': 0.2175, 'grad_norm': 0.7257044911384583, 'learning_rate': 4.68421052631579e-06, 'epoch': 3.7727272727272725}\n",
      "Step 420: {'loss': 0.2171, 'grad_norm': 1.056056022644043, 'learning_rate': 4.618421052631579e-06, 'epoch': 3.8181818181818183}\n",
      "Step 425: {'loss': 0.2255, 'grad_norm': 0.665004312992096, 'learning_rate': 4.552631578947369e-06, 'epoch': 3.8636363636363638}\n",
      "Step 430: {'loss': 0.2039, 'grad_norm': 0.699370801448822, 'learning_rate': 4.4868421052631584e-06, 'epoch': 3.909090909090909}\n",
      "Step 435: {'loss': 0.2475, 'grad_norm': 0.7404382824897766, 'learning_rate': 4.4210526315789476e-06, 'epoch': 3.9545454545454546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset/checkpoint-440\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-440/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-440/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 440: {'loss': 0.2425, 'grad_norm': 2.2097840309143066, 'learning_rate': 4.3552631578947375e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-440/model.safetensors\n",
      "Deleting older checkpoint [T5base_Question_Generation_v0_custom_dataset/checkpoint-330] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 445: {'loss': 0.2365, 'grad_norm': 0.8604252934455872, 'learning_rate': 4.289473684210527e-06, 'epoch': 4.045454545454546}\n",
      "Step 450: {'loss': 0.2212, 'grad_norm': 0.6776068806648254, 'learning_rate': 4.223684210526316e-06, 'epoch': 4.090909090909091}\n",
      "Step 455: {'loss': 0.2182, 'grad_norm': 0.7054225206375122, 'learning_rate': 4.157894736842106e-06, 'epoch': 4.136363636363637}\n",
      "Step 460: {'loss': 0.2631, 'grad_norm': 0.8247942328453064, 'learning_rate': 4.092105263157895e-06, 'epoch': 4.181818181818182}\n",
      "Step 465: {'loss': 0.2268, 'grad_norm': 0.5994593501091003, 'learning_rate': 4.026315789473684e-06, 'epoch': 4.2272727272727275}\n",
      "Step 470: {'loss': 0.2505, 'grad_norm': 0.8202710151672363, 'learning_rate': 3.960526315789474e-06, 'epoch': 4.2727272727272725}\n",
      "Step 475: {'loss': 0.2397, 'grad_norm': 1.394645094871521, 'learning_rate': 3.894736842105263e-06, 'epoch': 4.318181818181818}\n",
      "Step 480: {'loss': 0.2009, 'grad_norm': 1.5548375844955444, 'learning_rate': 3.828947368421053e-06, 'epoch': 4.363636363636363}\n",
      "Step 485: {'loss': 0.2313, 'grad_norm': 0.6870751976966858, 'learning_rate': 3.7631578947368426e-06, 'epoch': 4.409090909090909}\n",
      "Step 490: {'loss': 0.2171, 'grad_norm': 0.884809136390686, 'learning_rate': 3.6973684210526317e-06, 'epoch': 4.454545454545454}\n",
      "Step 495: {'loss': 0.2301, 'grad_norm': 0.7734841704368591, 'learning_rate': 3.6315789473684217e-06, 'epoch': 4.5}\n",
      "Step 500: {'loss': 0.2346, 'grad_norm': 0.8486359119415283, 'learning_rate': 3.565789473684211e-06, 'epoch': 4.545454545454545}\n",
      "Step 505: {'loss': 0.247, 'grad_norm': 1.9169847965240479, 'learning_rate': 3.5e-06, 'epoch': 4.590909090909091}\n",
      "Step 510: {'loss': 0.1941, 'grad_norm': 1.1611764430999756, 'learning_rate': 3.43421052631579e-06, 'epoch': 4.636363636363637}\n",
      "Step 515: {'loss': 0.2453, 'grad_norm': 0.8185659050941467, 'learning_rate': 3.368421052631579e-06, 'epoch': 4.681818181818182}\n",
      "Step 520: {'loss': 0.1875, 'grad_norm': 0.6614453196525574, 'learning_rate': 3.302631578947369e-06, 'epoch': 4.7272727272727275}\n",
      "Step 525: {'loss': 0.2495, 'grad_norm': 0.970474898815155, 'learning_rate': 3.236842105263158e-06, 'epoch': 4.7727272727272725}\n",
      "Step 530: {'loss': 0.1977, 'grad_norm': 0.6084712743759155, 'learning_rate': 3.1710526315789477e-06, 'epoch': 4.818181818181818}\n",
      "Step 535: {'loss': 0.2472, 'grad_norm': 0.8711532354354858, 'learning_rate': 3.1052631578947372e-06, 'epoch': 4.863636363636363}\n",
      "Step 540: {'loss': 0.2158, 'grad_norm': 1.0228487253189087, 'learning_rate': 3.0394736842105268e-06, 'epoch': 4.909090909090909}\n",
      "Step 545: {'loss': 0.2649, 'grad_norm': 1.2311499118804932, 'learning_rate': 2.973684210526316e-06, 'epoch': 4.954545454545455}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset/checkpoint-550\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-550/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-550/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 550: {'loss': 0.2431, 'grad_norm': 1.2193659543991089, 'learning_rate': 2.907894736842106e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-550/model.safetensors\n",
      "Deleting older checkpoint [T5base_Question_Generation_v0_custom_dataset/checkpoint-440] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 555: {'loss': 0.2556, 'grad_norm': 1.8953415155410767, 'learning_rate': 2.842105263157895e-06, 'epoch': 5.045454545454546}\n",
      "Step 560: {'loss': 0.216, 'grad_norm': 1.850293517112732, 'learning_rate': 2.776315789473684e-06, 'epoch': 5.090909090909091}\n",
      "Step 565: {'loss': 0.2127, 'grad_norm': 0.6361673474311829, 'learning_rate': 2.710526315789474e-06, 'epoch': 5.136363636363637}\n",
      "Step 570: {'loss': 0.2699, 'grad_norm': 0.7525569200515747, 'learning_rate': 2.644736842105263e-06, 'epoch': 5.181818181818182}\n",
      "Step 575: {'loss': 0.2175, 'grad_norm': 0.7845690846443176, 'learning_rate': 2.578947368421053e-06, 'epoch': 5.2272727272727275}\n",
      "Step 580: {'loss': 0.2264, 'grad_norm': 0.7046638131141663, 'learning_rate': 2.5131578947368423e-06, 'epoch': 5.2727272727272725}\n",
      "Step 585: {'loss': 0.2188, 'grad_norm': 0.8054198622703552, 'learning_rate': 2.447368421052632e-06, 'epoch': 5.318181818181818}\n",
      "Step 590: {'loss': 0.2323, 'grad_norm': 0.8321883082389832, 'learning_rate': 2.381578947368421e-06, 'epoch': 5.363636363636363}\n",
      "Step 595: {'loss': 0.2138, 'grad_norm': 0.9201245307922363, 'learning_rate': 2.3157894736842105e-06, 'epoch': 5.409090909090909}\n",
      "Step 600: {'loss': 0.2023, 'grad_norm': 0.8438060879707336, 'learning_rate': 2.25e-06, 'epoch': 5.454545454545454}\n",
      "Step 605: {'loss': 0.2242, 'grad_norm': 0.6272382736206055, 'learning_rate': 2.1842105263157896e-06, 'epoch': 5.5}\n",
      "Step 610: {'loss': 0.2292, 'grad_norm': 1.0608066320419312, 'learning_rate': 2.118421052631579e-06, 'epoch': 5.545454545454545}\n",
      "Step 615: {'loss': 0.1971, 'grad_norm': 0.6689320206642151, 'learning_rate': 2.0526315789473687e-06, 'epoch': 5.590909090909091}\n",
      "Step 620: {'loss': 0.2121, 'grad_norm': 0.5959572196006775, 'learning_rate': 1.9868421052631582e-06, 'epoch': 5.636363636363637}\n",
      "Step 625: {'loss': 0.2156, 'grad_norm': 0.7862231135368347, 'learning_rate': 1.9210526315789474e-06, 'epoch': 5.681818181818182}\n",
      "Step 630: {'loss': 0.2118, 'grad_norm': 0.8437899947166443, 'learning_rate': 1.855263157894737e-06, 'epoch': 5.7272727272727275}\n",
      "Step 635: {'loss': 0.1931, 'grad_norm': 0.5615730881690979, 'learning_rate': 1.7894736842105265e-06, 'epoch': 5.7727272727272725}\n",
      "Step 640: {'loss': 0.2336, 'grad_norm': 0.8312219977378845, 'learning_rate': 1.723684210526316e-06, 'epoch': 5.818181818181818}\n",
      "Step 645: {'loss': 0.2142, 'grad_norm': 0.8735613822937012, 'learning_rate': 1.6578947368421053e-06, 'epoch': 5.863636363636363}\n",
      "Step 650: {'loss': 0.2218, 'grad_norm': 0.737480878829956, 'learning_rate': 1.5921052631578949e-06, 'epoch': 5.909090909090909}\n",
      "Step 655: {'loss': 0.2218, 'grad_norm': 0.6855009198188782, 'learning_rate': 1.5263157894736844e-06, 'epoch': 5.954545454545455}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset/checkpoint-660\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-660/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-660/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 660: {'loss': 0.2426, 'grad_norm': 1.5828449726104736, 'learning_rate': 1.460526315789474e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-660/model.safetensors\n",
      "Deleting older checkpoint [T5base_Question_Generation_v0_custom_dataset/checkpoint-550] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 665: {'loss': 0.1788, 'grad_norm': 0.7968388199806213, 'learning_rate': 1.394736842105263e-06, 'epoch': 6.045454545454546}\n",
      "Step 670: {'loss': 0.2228, 'grad_norm': 0.826279878616333, 'learning_rate': 1.3289473684210526e-06, 'epoch': 6.090909090909091}\n",
      "Step 675: {'loss': 0.1887, 'grad_norm': 0.8203238844871521, 'learning_rate': 1.2631578947368422e-06, 'epoch': 6.136363636363637}\n",
      "Step 680: {'loss': 0.1954, 'grad_norm': 0.7113037109375, 'learning_rate': 1.1973684210526317e-06, 'epoch': 6.181818181818182}\n",
      "Step 685: {'loss': 0.2101, 'grad_norm': 0.6687749624252319, 'learning_rate': 1.1315789473684213e-06, 'epoch': 6.2272727272727275}\n",
      "Step 690: {'loss': 0.2253, 'grad_norm': 1.1908609867095947, 'learning_rate': 1.0657894736842106e-06, 'epoch': 6.2727272727272725}\n",
      "Step 695: {'loss': 0.2224, 'grad_norm': 2.305074453353882, 'learning_rate': 1.0000000000000002e-06, 'epoch': 6.318181818181818}\n",
      "Step 700: {'loss': 0.2064, 'grad_norm': 0.7772131562232971, 'learning_rate': 9.342105263157895e-07, 'epoch': 6.363636363636363}\n",
      "Step 705: {'loss': 0.2402, 'grad_norm': 0.7918450236320496, 'learning_rate': 8.68421052631579e-07, 'epoch': 6.409090909090909}\n",
      "Step 710: {'loss': 0.198, 'grad_norm': 0.9281554222106934, 'learning_rate': 8.026315789473685e-07, 'epoch': 6.454545454545454}\n",
      "Step 715: {'loss': 0.2228, 'grad_norm': 1.0220791101455688, 'learning_rate': 7.368421052631579e-07, 'epoch': 6.5}\n",
      "Step 720: {'loss': 0.2376, 'grad_norm': 0.9861708283424377, 'learning_rate': 6.710526315789475e-07, 'epoch': 6.545454545454545}\n",
      "Step 725: {'loss': 0.2429, 'grad_norm': 0.8549965620040894, 'learning_rate': 6.052631578947369e-07, 'epoch': 6.590909090909091}\n",
      "Step 730: {'loss': 0.1806, 'grad_norm': 0.5891911387443542, 'learning_rate': 5.394736842105264e-07, 'epoch': 6.636363636363637}\n",
      "Step 735: {'loss': 0.2327, 'grad_norm': 0.7730823159217834, 'learning_rate': 4.7368421052631585e-07, 'epoch': 6.681818181818182}\n",
      "Step 740: {'loss': 0.2317, 'grad_norm': 0.7894598841667175, 'learning_rate': 4.078947368421053e-07, 'epoch': 6.7272727272727275}\n",
      "Step 745: {'loss': 0.1993, 'grad_norm': 0.6705653071403503, 'learning_rate': 3.421052631578948e-07, 'epoch': 6.7727272727272725}\n",
      "Step 750: {'loss': 0.1952, 'grad_norm': 1.0241755247116089, 'learning_rate': 2.763157894736842e-07, 'epoch': 6.818181818181818}\n",
      "Step 755: {'loss': 0.2013, 'grad_norm': 0.5886897444725037, 'learning_rate': 2.105263157894737e-07, 'epoch': 6.863636363636363}\n",
      "Step 760: {'loss': 0.2444, 'grad_norm': 0.8843564391136169, 'learning_rate': 1.4473684210526316e-07, 'epoch': 6.909090909090909}\n",
      "Step 765: {'loss': 0.1861, 'grad_norm': 0.7678730487823486, 'learning_rate': 7.894736842105264e-08, 'epoch': 6.954545454545455}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset/checkpoint-770\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-770/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-770/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 770: {'loss': 0.2058, 'grad_norm': 1.4775458574295044, 'learning_rate': 1.3157894736842106e-08, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/checkpoint-770/model.safetensors\n",
      "Deleting older checkpoint [T5base_Question_Generation_v0_custom_dataset/checkpoint-660] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 770: {'train_runtime': 218.3065, 'train_samples_per_second': 10.517, 'train_steps_per_second': 3.527, 'total_flos': 1398167316725760.0, 'train_loss': 1.2984254661318544, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base_Question_Generation_v0_custom_dataset\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/config.json\n",
      "Configuration saved in ./T5base_Question_Generation_v0_custom_dataset/generation_config.json\n",
      "Model weights saved in ./T5base_Question_Generation_v0_custom_dataset/model.safetensors\n",
      "tokenizer config file saved in ./T5base_Question_Generation_v0_custom_dataset/tokenizer_config.json\n",
      "Special tokens file saved in ./T5base_Question_Generation_v0_custom_dataset/special_tokens_map.json\n",
      "added tokens file saved in ./T5base_Question_Generation_v0_custom_dataset/added_tokens.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./T5base_Question_Generation_v0_custom_dataset/tokenizer_config.json',\n",
       " './T5base_Question_Generation_v0_custom_dataset/special_tokens_map.json',\n",
       " './T5base_Question_Generation_v0_custom_dataset/spiece.model',\n",
       " './T5base_Question_Generation_v0_custom_dataset/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(save_path)                           \n",
    "\n",
    "tokenizer.save_pretrained(save_path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7727136-4ae6-444a-8a15-0126438733cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6e735-34b5-4ab5-8793-65e22a234a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a144b-8279-47a4-b3e4-529284dcac56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c67f0332-d6a4-48af-a237-98e0c9547f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading file chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file ./T5base_Question_Generation_v0_custom_dataset/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./T5base_Question_Generation_v0_custom_dataset/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./T5base_Question_Generation_v0_custom_dataset.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./T5base_Question_Generation_v0_custom_dataset/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the finetuned question generation model with a sample context\n",
    "\n",
    "# the finetuned model name\n",
    "new_model = \"./T5base_Question_Generation_v0_custom_dataset\"\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(new_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(new_model)\n",
    "\n",
    "\n",
    "def get_question(tag, difficulty, context, answer=\"\", num_questions=3, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate questions using the fine-tuned T5 model.\n",
    "    \n",
    "    Parameters:\n",
    "    - tag: Type of question (e.g., \"short answer\", \"multiple choice question\", \"true or false question\")\n",
    "    - difficulty: \"easy\", \"medium\", \"hard\"\n",
    "    - context: Supporting context or passage\n",
    "    - answer: Optional — if you want targeted question generation\n",
    "    - num_questions: Number of diverse questions to generate\n",
    "    - max_length: Max token length of generated output\n",
    "    \n",
    "    Returns:\n",
    "    - List of generated questions as strings\n",
    "    \"\"\"\n",
    "    # Format input text based on whether answer is provided\n",
    "    answer_part = f\"[{answer}]\" if answer else \"\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{answer_part} {context}\"\n",
    "\n",
    "    # Tokenize\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    # Generate questions\n",
    "    output = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        max_length=max_length,\n",
    "\n",
    "        # Beam Search - just prints only one question\n",
    "        # num_beams = 5,\n",
    "        # early_stopping=True,              # to stop when the first beam is finished \n",
    "\n",
    "        # Sampling\n",
    "        num_return_sequences=num_questions,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    # Decode generated questions\n",
    "    for i, out in enumerate(output):\n",
    "        question = tokenizer.decode(out, skip_special_tokens=True)\n",
    "        print(f\"Question {i+1}: {question}\")\n",
    "    \n",
    "    print(\"------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bebe9ad-1353-48c7-9360-27c169511aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving the context and difficulty\n",
    "\n",
    "\n",
    "context = \"Reinforcement Learning (RL) is a dynamic area of machine learning where agents are trained to make a sequence of decisions by interacting with an environment. Each interaction leads to a new state and a scalar reward, which indicates the quality of the action taken. The agent’s objective is to learn an optimal policy that maximizes the total accumulated reward over time. This is different from supervised learning, which requires labeled datasets. In RL, learning is driven by experience and the agent often learns from delayed rewards, making the credit assignment problem a central challenge. The environment is often modeled as a Markov Decision Process (MDP), characterized by states, actions, transition dynamics, and rewards. Algorithms such as Q-learning, SARSA, and Policy Gradient methods are used to find optimal policies. Modern applications employ deep learning to approximate complex functions, giving rise to Deep Reinforcement Learning. Techniques like Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Actor-Critic methods have demonstrated state-of-the-art performance in domains ranging from game playing (e.g., Atari, Go) to robotics and recommendation systems. Exploration-exploitation trade-offs, sample efficiency, and generalization are ongoing challenges in the field. RL has significant potential in real-world decision-making systems.\"\n",
    "\n",
    "\n",
    "difficulty = \"hard\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49c1ea99-ea8d-4cd7-b027-8c2f0538d735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: \n",
      "Question 2: \n",
      "Question 3: \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# short answer question \n",
    "\n",
    "get_question(\n",
    "    tag=\"long answer question\",\n",
    "    difficulty=difficulty,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "629cbe70-6ee0-45da-ad43-baa2d64b1d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# true or false question \n",
    "\n",
    "get_question(\n",
    "    tag=\"true or false question\",\n",
    "    difficulty=difficulty,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c20b47a3-8e5e-474b-82ee-f6f90c07a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# multiple choice question \n",
    "\n",
    "get_question(\n",
    "    tag=\"multiple choice question\",\n",
    "    difficulty=difficulty,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064d5bf-7457-46d6-bc8e-a34cc0b94608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
