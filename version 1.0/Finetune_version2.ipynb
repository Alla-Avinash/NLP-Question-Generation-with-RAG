{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0985e50-7d25-4377-97b1-a1e1d3261635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the directory where we want to store the models\n",
    "import os\n",
    "\n",
    "print(\"Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - \" + os.getcwd())\n",
    "os.environ['HF_HOME'] = \"/home/aalla4\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/home/aalla4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b6172-dae2-4185-9fe8-d265344e8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# getting the CPU and GPU count\n",
    "\n",
    "print(\"Total number of logical cores = \" + str(os.cpu_count()))  # This shows logical cores not the physical cores\n",
    "LOGICAL_CORES = os.cpu_count()\n",
    "USABLE_CPU_CORES = LOGICAL_CORES - 2    # YOU CAN CHANGE THIS ACCORDING TO THE CPU AVAILABILITIES\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c4b4a-8afd-470d-9955-bae54bcae0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer \n",
    "\n",
    "model_name = \"google/flan-t5-base\" \n",
    "\n",
    "save_path = \"./T5base_Question_Generation_v2\"\n",
    "tokenized_trainset_save_path = \"./dataset/individual/boolq/tokenized_trainset.parquet\"\n",
    "tokenized_valset_save_path = \"./dataset/individual/boolq/tokenized_valset.parquet\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Max positional embeddings supported by model - {model_name}: \", model.config.n_positions)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f2536-4010-49b0-b370-db441a5226b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "# num_training_examples = 4500       # to maintain uniformity when we mix all the various datasets, to get the model to understand the structure of different questions like mcq, t/f, short ans questions\n",
    "\n",
    "# datasets used are squad v1, hotpotqa, openbookqa, drop, boolq\n",
    "# the datasets returns a DatasetDict with \"train\" and \"validation\" splits\n",
    "\n",
    "print(\"Loading the datasets ........\")\n",
    "\n",
    "# Load HotpotQA (distractor)\n",
    "hotpotqa = load_dataset(\"hotpot_qa\", \"distractor\",  trust_remote_code=True)\n",
    "\n",
    "# SQUAD (V1)\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# OpenBookQA (additional)\n",
    "openbookqa = load_dataset(\"openbookqa\", \"additional\")\n",
    "\n",
    "# Boolq dataset\n",
    "boolq = load_dataset(\"boolq\")\n",
    "\n",
    "# Drop dataset\n",
    "drop = load_dataset(\"drop\")\n",
    "\n",
    "print(\"Completed loading the datasets ........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fc730-1ab0-4511-85db-ff469e7dfde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions of the datasets\n",
    "\n",
    "# Preprocessing, combining and shuffling datasets only uses LOGICAL_CORES/ CPU's and not done on GPU's\n",
    "print(\"Starting Preprocesing; Available CPU Cores to use - \"+ str(USABLE_CPU_CORES))\n",
    "\n",
    "def preprocess_hotpotqa(example):  \n",
    "    \"hotpotq - used for short answer question generation, with focus on multi-hop sentences rater than forming the question with just a single line in the whole context\"      \n",
    "\n",
    "    answer = example['answer']\n",
    "    if len(answer.split()) < 12:                    \n",
    "        tag = \"very short answer\"\n",
    "    else:\n",
    "        tag = \"short answer\"\n",
    "    difficulty = example['level']                   # Extract difficulty - 'easy'/ 'medium'/ 'hard'\n",
    "\n",
    "    # thought of just keeping the supporting sentences instead of the all the sentences in the supporting titles, but the context length is too short for this approach           \n",
    "    supporting_facts = example[\"supporting_facts\"]\n",
    "    supporting_titles = set([t for t in supporting_facts['title']])          # Use set to avoid duplicates\n",
    "    context_titles = example['context']['title']\n",
    "    context_sentences = example['context']['sentences']\n",
    "\n",
    "    supporting_sentences = []\n",
    "\n",
    "    for idx, title in enumerate(context_titles):\n",
    "        if title in supporting_titles:\n",
    "            # Add all sentences under this title\n",
    "            req_sentences = []\n",
    "            for sent in context_sentences[idx]:\n",
    "                req_sentences.append(f\"{sent}\")\n",
    "            sentence_block = \" \".join(req_sentences)  \n",
    "            supporting_sentences.append(f\"{title}, {sentence_block}\")\n",
    "\n",
    "    short_context = \" \".join(supporting_sentences)\n",
    "    \n",
    "    # Randomly decide whether to include the answer or not as not always the answer will be likely to be provided by the instructor\n",
    "    include_answer = random.choices([True, False], weights=[15, 85], k=1)[0]\n",
    "\n",
    "    if include_answer:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>[{answer}] {short_context}\"     # Prepare the model input\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>[{answer}] {short_context}\"\n",
    "    else:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{short_context}\"\n",
    "       \n",
    "    target_text = f\"{example['question']}\"                                                                       # Prepare the target output\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text) \n",
    "    \n",
    "    # Tokenize both input and output\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Attach labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_squad(example):\n",
    "    \"squad - used for short answer question generation, with multiple questions generated with the same context\"\n",
    "    \n",
    "    answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "    if len(answer.split()) < 12:\n",
    "        tag = \"very short answer\"\n",
    "    else:\n",
    "        tag = \"short answer\"\n",
    "    difficulty = \"easy\"\n",
    "    context = example['context']\n",
    "\n",
    "    include_answer = random.choices([True, False], weights=[15, 85], k=1)[0]\n",
    "    if include_answer:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>[{answer}] {context}\"\n",
    "    else:\n",
    "        input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{context}\"\n",
    "\n",
    "    target_text = example[\"question\"]\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_openbookqa(example):\n",
    "    \"openbookqa - used for multiple choice question generation\"\n",
    "   \n",
    "    # difficulty is calculated based on the human score and clarity\n",
    "    clarity = example.get(\"clarity\", 0)\n",
    "    human_score = example.get(\"human_score\", 1)\n",
    "    \n",
    "    # Filter out low clarity examples\n",
    "    # if clarity <= 1:\n",
    "    #     return None       # discard low-clarity / low-quality examples\n",
    "        # return {}\n",
    "\n",
    "    # Assign difficulty\n",
    "    if clarity > 1.8 and human_score < 1:\n",
    "        difficulty = \"hard\" \n",
    "    else:                             # [ 1 < clarity <= 1.8 ] and if [ clarity > 1.8 and human_score > 1 ]\n",
    "        difficulty = \"medium\"\n",
    "\n",
    "    tag = \"multiple choice question\"\n",
    "    \n",
    "    # Build the context using fact1 (from additional)\n",
    "    fact1 = example.get(\"fact1\", \"\")                # example.get(\"fact1\", \"\") is safe â€” it gives you a default value if the key is missing, example[\"fact1\"] will raise a KeyError if 'fact1' is missing.\n",
    "    \n",
    "    # Construct the multiple choice question format with answer marked\n",
    "    question_stem = example[\"question_stem\"]\n",
    "    answer_key = example[\"answerKey\"]              # 'A', 'B', 'C', or 'D'\n",
    "    choices = example[\"choices\"][\"text\"]\n",
    "    options = example[\"choices\"][\"label\"]            # ['A', 'B', 'C', 'D']\n",
    "    # choices_str = \"; \".join(example[\"choices\"][\"text\"])\n",
    "    choices_str = \"; \".join([f\"{option}. {text}\" for option, text in zip(options, choices)])\n",
    "    # answer_text = choices[\"text\"][int(ord(answer_key) - ord('A'))]\n",
    "    answer_text = choices[int(ord(answer_key) - ord('A'))]\n",
    "\n",
    "    target_text = f\"{question_stem}: [{answer_text}] ({choices_str})\"\n",
    "\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{fact1}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"clarity\"] = clarity                 # Add clarity for later filtering\n",
    "    model_inputs[\"answer\"] = answer_text\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_boolq(example):\n",
    "    \"boolq - used for true/ false question generation\"\n",
    "\n",
    "    # difficulty tagging is not available in BoolQ, so we default it\n",
    "    difficulty = \"medium\"\n",
    "    tag = \"true or false question\"\n",
    "\n",
    "    # Extract question and answer\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]          # this is a bool value: True / False\n",
    "    passage = example[\"passage\"]\n",
    "\n",
    "    target_text = f\"{question} [{'true' if answer else 'false'}]\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{passage}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_drop(example):\n",
    "    \"drop - used for very short answer question generation, with multiple questions generated with the same context, it  also includes some math calculations that has to be done by the model\"\n",
    "\n",
    "    tag = \"one word answer\"\n",
    "    difficulty = \"medium\"\n",
    "    question = example.get(\"question\", \"\")\n",
    "    passage = example.get(\"passage\", \"\")\n",
    "    \n",
    "    # Retrieve the first available answer from 'answers_spans' (string or list of strings)\n",
    "    answers = example.get(\"answers_spans\", {}).get(\"spans\", [])\n",
    "    if not answers:\n",
    "        return None                                           # Skip if no answer available\n",
    "\n",
    "    answer = answers[0]      \n",
    "    \n",
    "    target_text = f\"{question} [{answer}]\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{passage}\"\n",
    "\n",
    "    # print(input_text, \"\\n\", target_text)\n",
    "\n",
    "    # Tokenization\n",
    "    model_inputs = tokenizer(input_text, max_length=tokenizer_input_max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=tokenizer_label_max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea717e7-e6f4-4f1c-b440-4a5c57d51861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into train and validation & Preprocessing the datasets individually\n",
    "\n",
    "# Preprocess both splits separately for both datasets\n",
    "# put the .select(range(5)) steps for testing out the input and output results and printing them ******\n",
    "# hotpotqa_train = hotpotqa[\"train\"].select(range(5)).map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES)\n",
    "# hotpotqa_train = hotpotqa[\"train\"].select(range(5)).map(preprocess_hotpotqa)\n",
    "\n",
    "# trail\n",
    "# drop_train = drop[\"train\"].select(range(5)).map(preprocess_drop)\n",
    "# drop_val = drop[\"validation\"].select(range(5)).map(preprocess_drop)\n",
    "# boolq_train = boolq[\"train\"].select(range(5)).map(preprocess_boolq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4418b89-8e48-49d3-b74e-3090370124db",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa_train = hotpotqa[\"train\"].select(range(5)).map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=hotpotqa[\"train\"].column_names)\n",
    "hotpotqa_val = hotpotqa[\"validation\"].map(preprocess_hotpotqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=hotpotqa[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3524c-3889-48ad-a51a-c58e5cf4abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = squad[\"train\"].select(range(num_training_examples)).map(preprocess_squad, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=squad[\"train\"].column_names)\n",
    "squad_val = squad[\"validation\"].map(preprocess_squad, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=squad[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eebb9e-b5e6-4541-9d2f-3ff6ef5fcd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "openbookqa_train = openbookqa[\"train\"].select(range(num_training_examples)).map(preprocess_openbookqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=openbookqa[\"train\"].column_names).filter( lambda x: (x.get(\"clarity\", 0) > 1) and bool(x.get(\"answer\"))).remove_columns([\"clarity\", \"answer\"])\n",
    "openbookqa_val = openbookqa[\"validation\"].map(preprocess_openbookqa, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=openbookqa[\"validation\"].column_names).filter( lambda x: (x.get(\"clarity\", 0) > 1) and bool(x.get(\"answer\"))).remove_columns([\"clarity\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f37a5c-27a2-493f-a98f-9f3c934e3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_train = boolq[\"train\"].map(preprocess_boolq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=boolq[\"train\"].column_names)\n",
    "boolq_val = boolq[\"validation\"].map(preprocess_boolq, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=boolq[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb792db9-abcb-4a3c-87ea-2083c2df7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = drop[\"train\"].select(range(num_training_examples)).map(preprocess_drop, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=drop[\"train\"].column_names)\n",
    "drop_val = drop[\"validation\"].map(preprocess_drop, batched=False, num_proc=USABLE_CPU_CORES, remove_columns=drop[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d73950-47f9-493b-b98d-2542b70c3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the datasets and shuffling them\n",
    "\n",
    "# Combine and shuffle the datasets\n",
    "# tokenized_train_dataset = concatenate_datasets([\n",
    "#     hotpotqa_train,\n",
    "#     squad_train,\n",
    "#     openbookqa_train,\n",
    "#     boolq_train,\n",
    "#     drop_train\n",
    "# ]).shuffle(seed=92)\n",
    "\n",
    "# tokenized_val_dataset = concatenate_datasets([\n",
    "#     hotpotqa_val,\n",
    "#     squad_val,\n",
    "#     openbookqa_val,\n",
    "#     boolq_val,\n",
    "#     drop_val\n",
    "# ]).shuffle(seed=42)\n",
    "\n",
    "tokenized_train_dataset = boolq_train\n",
    "tokenized_val_dataset = boolq_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e113d-c5bf-488b-bdd4-da83affa2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet (efficient for large datasets)\n",
    "\n",
    "tokenized_train_dataset.to_parquet(tokenized_trainset_save_path)\n",
    "tokenized_val_dataset.to_parquet(tokenized_valset_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700bbfd4-7fa9-4a41-aa62-638b7cfb345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ce9b9-652a-4523-9879-cf4e4724ba84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8b6d8-dc0e-4dc7-bb43-2819b7e97cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d59e9-5efd-41c2-81ae-ce58eec21d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b998b-62d9-4382-b6f8-95b1bd344119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92605cc3-ff15-4db8-bc03-be8ffddc6533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11261e74-f65d-4994-ac2b-c69440dd9bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - /home/aalla4\n"
     ]
    }
   ],
   "source": [
    "# setting up the directory where we want to store the models\n",
    "import os\n",
    "\n",
    "print(\"Setting up the current working directory as the place where to host the transformers models downloaded from hugging face - \" + os.getcwd())\n",
    "os.environ['HF_HOME'] = \"/home/aalla4\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/home/aalla4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72894351-5c30-49f9-814c-83d999adfec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f45352e-0afa-403f-bad6-dcd19861e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of logical cores = 48\n",
      "CUDA available:  True\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# getting the CPU and GPU count\n",
    "\n",
    "print(\"Total number of logical cores = \" + str(os.cpu_count()))  # This shows logical cores not the physical cores\n",
    "LOGICAL_CORES = os.cpu_count()\n",
    "USABLE_CPU_CORES = LOGICAL_CORES - 1    # YOU CAN CHANGE THIS ACCORDING TO THE CPU AVAILABILITIES\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df23a6c-1371-48b8-95ee-1c43bced2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max positional embeddings supported by model - google/flan-t5-base:  512\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer \n",
    "\n",
    "model_name = \"google/flan-t5-base\" \n",
    "\n",
    "# new model save path\n",
    "save_path = \"./T5base-flan-lora-adapter-custom-dataset\"\n",
    "\n",
    "tokenized_trainset_path = \"./dataset/custom_dataset/tokenized_trainset.parquet\"\n",
    "# tokenized_valset_path = \"./dataset/individual/hotpotqa/tokenized_valset.parquet\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False, device_map=\"auto\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Max positional embeddings supported by model - {model_name}: \", model.config.n_positions)\n",
    "\n",
    "tokenizer_input_max_length = 512\n",
    "tokenizer_label_max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76675cb-7f12-43f5-ab37-4f53b1ad14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenized Dataset if Preprocessing is already done\n",
    "\n",
    "# Load from Parquet\n",
    "tokenized_train_dataset = Dataset.from_parquet(tokenized_trainset_path)\n",
    "# tokenized_val_dataset = Dataset.from_parquet(tokenized_valset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e893ecbc-ed8d-4059-93ad-b176eaa65bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                         # LoRA rank\n",
    "    lora_alpha=16,               # LoRA scaling factor\n",
    "    target_modules=[\"q\", \"v\"],   # Typically for T5: q and v projections\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1404b73c-c84a-4f71-8a53-069ad4b325e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the current device:  cuda\n",
      "Device updated to: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Defning the Training args \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"This is the current device: \", device)\n",
    "print(\"Device updated to:\", model.device)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,              # directory where the training logs, checkpoints, and evaluation results (like metrics) are saved during the training process   \n",
    "    \n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=5,                         \n",
    "    weight_decay=1e-3,                          \n",
    "\n",
    "    # generally Batch size = per_device_train_batch_size * per_device_train_batch_size\n",
    "    per_device_train_batch_size=4,                       # ****** if using GPU's = 16; if using CPU's = 1 or 2\n",
    "    # per_device_eval_batch_size=8,\n",
    "    # gradient_accumulation_steps=2,                     # Increase this if you need to simulate larger batch sizes, without running into 'Out or Memory' errors when memory is limited\n",
    "    \n",
    "\n",
    "    # this is not useful for CPU based training as hugging face trainer handles multi-core utilization automatically based on the system configuration\n",
    "    dataloader_num_workers= USABLE_CPU_CORES,          # ****** for optimal use of CPU and not wasting GPU time [ this helps in loading the next batch of data into the VRAM ]\n",
    "\n",
    "    # Print validation loss every epoch\n",
    "    # eval_strategy=\"epoch\",            \n",
    "\n",
    "    # Print and logs the training loss of the training data\n",
    "    logging_strategy=\"steps\",   \n",
    "    logging_steps=10,                                   # ****** if using GPU = 100; if using CPU = 1 or 2 \n",
    "\n",
    "    # saves model at the end of every epoch\n",
    "    save_strategy=\"epoch\",            \n",
    "    save_total_limit=1,\n",
    "    # save_total_limit=2,\n",
    "\n",
    "    # report_to=\"none\",  # Disable default logging\n",
    "    \n",
    "    logging_dir= save_path + \"/logs\",           # save logs to a directory\n",
    "    report_to=\"tensorboard\",                    # Reports to TensorBoard\n",
    "    log_level='info',                           # Set logging level to 'info' to see the logs in the terminal\n",
    "    # run this command in your terminal ~ tensorboard --logdir=./output_dir/runs\n",
    "    # and open 'http://localhost:6006/' to monitor the logs [loss over the training]\n",
    "\n",
    "    fp16=False                                   # ***** Mixed precision for faster training on A100; this won't work on CPU's\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d38f287-03b3-478d-b17c-030f20e27c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging in the Terminal\n",
    "class LogCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step {state.global_step}: {logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e91dfd4-31b8-4a6d-8d3d-cd8b5c4fd6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=tokenized_train_dataset,            # your test set\n",
    "    # eval_dataset=tokenized_val_dataset,               # your validation set\n",
    "\n",
    "    # train_dataset=tokenized_train_dataset.select(range(100)),    \n",
    "    # eval_dataset=tokenized_val_dataset.select(range(100)),\n",
    "    \n",
    "    callbacks=[LogCallback()]                   # *** to print the logs in the terminal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b222725c-75e8-4145-bd00-8602a85f73da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 328\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 410\n",
      "  Number of trainable parameters = 884,736\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [410/410 01:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>47.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>46.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>45.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>45.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>46.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>46.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>46.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>45.757700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>45.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>44.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>44.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>43.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>44.712400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>43.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>43.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>44.607900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>44.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>44.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>44.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>42.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>42.276700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>43.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>42.916400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>43.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>42.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>44.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>42.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>44.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>42.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>42.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>43.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>43.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>41.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>42.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>41.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>42.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>42.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>42.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>43.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>41.538900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>42.664500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: {'loss': 47.2601, 'grad_norm': 19.015579223632812, 'learning_rate': 9e-06, 'epoch': 0.12195121951219512}\n",
      "Step 20: {'loss': 46.8314, 'grad_norm': 15.072036743164062, 'learning_rate': 9.775e-06, 'epoch': 0.24390243902439024}\n",
      "Step 30: {'loss': 45.1818, 'grad_norm': 29.005718231201172, 'learning_rate': 9.525000000000001e-06, 'epoch': 0.36585365853658536}\n",
      "Step 40: {'loss': 45.7363, 'grad_norm': 27.39539337158203, 'learning_rate': 9.275e-06, 'epoch': 0.4878048780487805}\n",
      "Step 50: {'loss': 46.4363, 'grad_norm': 16.72978973388672, 'learning_rate': 9.025e-06, 'epoch': 0.6097560975609756}\n",
      "Step 60: {'loss': 46.5481, 'grad_norm': 23.206083297729492, 'learning_rate': 8.775e-06, 'epoch': 0.7317073170731707}\n",
      "Step 70: {'loss': 46.0279, 'grad_norm': 23.724437713623047, 'learning_rate': 8.525e-06, 'epoch': 0.8536585365853658}\n",
      "Step 80: {'loss': 45.7577, 'grad_norm': 40.15237045288086, 'learning_rate': 8.275000000000001e-06, 'epoch': 0.975609756097561}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base-flan-lora-adapter-custom-dataset/checkpoint-82\n",
      "loading configuration file config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90: {'loss': 45.7576, 'grad_norm': 14.941641807556152, 'learning_rate': 8.025e-06, 'epoch': 1.0975609756097562}\n",
      "Step 100: {'loss': 44.8286, 'grad_norm': 41.72953796386719, 'learning_rate': 7.775000000000001e-06, 'epoch': 1.2195121951219512}\n",
      "Step 110: {'loss': 44.7675, 'grad_norm': 115.38069915771484, 'learning_rate': 7.525e-06, 'epoch': 1.3414634146341464}\n",
      "Step 120: {'loss': 43.9772, 'grad_norm': 35.55817413330078, 'learning_rate': 7.275000000000001e-06, 'epoch': 1.4634146341463414}\n",
      "Step 130: {'loss': 44.7124, 'grad_norm': 19.518259048461914, 'learning_rate': 7.0250000000000005e-06, 'epoch': 1.5853658536585367}\n",
      "Step 140: {'loss': 43.0448, 'grad_norm': 23.692852020263672, 'learning_rate': 6.775e-06, 'epoch': 1.7073170731707317}\n",
      "Step 150: {'loss': 43.8796, 'grad_norm': 19.552507400512695, 'learning_rate': 6.525e-06, 'epoch': 1.8292682926829267}\n",
      "Step 160: {'loss': 44.6079, 'grad_norm': 32.34619140625, 'learning_rate': 6.275e-06, 'epoch': 1.951219512195122}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base-flan-lora-adapter-custom-dataset/checkpoint-164\n",
      "loading configuration file config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Deleting older checkpoint [T5base-flan-lora-adapter-custom-dataset/checkpoint-82] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 170: {'loss': 44.3425, 'grad_norm': 30.581275939941406, 'learning_rate': 6.025000000000001e-06, 'epoch': 2.073170731707317}\n",
      "Step 180: {'loss': 44.9565, 'grad_norm': 461.8547668457031, 'learning_rate': 5.775000000000001e-06, 'epoch': 2.1951219512195124}\n",
      "Step 190: {'loss': 44.3882, 'grad_norm': 15.751220703125, 'learning_rate': 5.5250000000000005e-06, 'epoch': 2.317073170731707}\n",
      "Step 200: {'loss': 42.0071, 'grad_norm': 45.706363677978516, 'learning_rate': 5.275e-06, 'epoch': 2.4390243902439024}\n",
      "Step 210: {'loss': 42.2767, 'grad_norm': 15.913215637207031, 'learning_rate': 5.025e-06, 'epoch': 2.5609756097560976}\n",
      "Step 220: {'loss': 43.873, 'grad_norm': 57.85839080810547, 'learning_rate': 4.775e-06, 'epoch': 2.682926829268293}\n",
      "Step 230: {'loss': 42.9164, 'grad_norm': 36.830379486083984, 'learning_rate': 4.525000000000001e-06, 'epoch': 2.8048780487804876}\n",
      "Step 240: {'loss': 43.0955, 'grad_norm': 22.85969352722168, 'learning_rate': 4.2750000000000006e-06, 'epoch': 2.926829268292683}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base-flan-lora-adapter-custom-dataset/checkpoint-246\n",
      "loading configuration file config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Deleting older checkpoint [T5base-flan-lora-adapter-custom-dataset/checkpoint-164] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250: {'loss': 42.0319, 'grad_norm': 46.067501068115234, 'learning_rate': 4.0250000000000004e-06, 'epoch': 3.048780487804878}\n",
      "Step 260: {'loss': 44.5769, 'grad_norm': 35.68975067138672, 'learning_rate': 3.7750000000000003e-06, 'epoch': 3.1707317073170733}\n",
      "Step 270: {'loss': 42.709, 'grad_norm': 24.773191452026367, 'learning_rate': 3.525e-06, 'epoch': 3.292682926829268}\n",
      "Step 280: {'loss': 44.4552, 'grad_norm': 56.44078063964844, 'learning_rate': 3.2750000000000004e-06, 'epoch': 3.4146341463414633}\n",
      "Step 290: {'loss': 42.2936, 'grad_norm': 47.7620849609375, 'learning_rate': 3.0250000000000003e-06, 'epoch': 3.5365853658536586}\n",
      "Step 300: {'loss': 42.5762, 'grad_norm': 28.907278060913086, 'learning_rate': 2.7750000000000005e-06, 'epoch': 3.658536585365854}\n",
      "Step 310: {'loss': 43.0203, 'grad_norm': 23.486928939819336, 'learning_rate': 2.5250000000000004e-06, 'epoch': 3.7804878048780486}\n",
      "Step 320: {'loss': 43.6126, 'grad_norm': 63.90212631225586, 'learning_rate': 2.2750000000000002e-06, 'epoch': 3.902439024390244}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base-flan-lora-adapter-custom-dataset/checkpoint-328\n",
      "loading configuration file config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Deleting older checkpoint [T5base-flan-lora-adapter-custom-dataset/checkpoint-246] due to args.save_total_limit\n",
      "/home/aalla4/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 47 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 330: {'loss': 41.1653, 'grad_norm': 18.159597396850586, 'learning_rate': 2.025e-06, 'epoch': 4.024390243902439}\n",
      "Step 340: {'loss': 42.5773, 'grad_norm': 21.777467727661133, 'learning_rate': 1.7750000000000002e-06, 'epoch': 4.146341463414634}\n",
      "Step 350: {'loss': 41.805, 'grad_norm': 13.325637817382812, 'learning_rate': 1.525e-06, 'epoch': 4.2682926829268295}\n",
      "Step 360: {'loss': 42.1247, 'grad_norm': 16.249387741088867, 'learning_rate': 1.275e-06, 'epoch': 4.390243902439025}\n",
      "Step 370: {'loss': 42.7134, 'grad_norm': 43.47506332397461, 'learning_rate': 1.025e-06, 'epoch': 4.512195121951219}\n",
      "Step 380: {'loss': 42.0563, 'grad_norm': 26.96151351928711, 'learning_rate': 7.750000000000001e-07, 'epoch': 4.634146341463414}\n",
      "Step 390: {'loss': 43.2065, 'grad_norm': 28.312908172607422, 'learning_rate': 5.250000000000001e-07, 'epoch': 4.7560975609756095}\n",
      "Step 400: {'loss': 41.5389, 'grad_norm': 13.661434173583984, 'learning_rate': 2.75e-07, 'epoch': 4.878048780487805}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base-flan-lora-adapter-custom-dataset/checkpoint-410\n",
      "loading configuration file config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 410: {'loss': 42.6645, 'grad_norm': 22.196388244628906, 'learning_rate': 2.5000000000000002e-08, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [T5base-flan-lora-adapter-custom-dataset/checkpoint-328] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 410: {'train_runtime': 72.3832, 'train_samples_per_second': 22.657, 'train_steps_per_second': 5.664, 'total_flos': 1127459428761600.0, 'train_loss': 43.86192314334032, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./T5base-flan-lora-adapter-custom-dataset\n",
      "loading configuration file config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./T5base-flan-lora-adapter-custom-dataset/tokenizer_config.json\n",
      "Special tokens file saved in ./T5base-flan-lora-adapter-custom-dataset/special_tokens_map.json\n",
      "added tokens file saved in ./T5base-flan-lora-adapter-custom-dataset/added_tokens.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./T5base-flan-lora-adapter-custom-dataset/tokenizer_config.json',\n",
       " './T5base-flan-lora-adapter-custom-dataset/special_tokens_map.json',\n",
       " './T5base-flan-lora-adapter-custom-dataset/spiece.model',\n",
       " './T5base-flan-lora-adapter-custom-dataset/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(save_path)                           \n",
    "\n",
    "tokenizer.save_pretrained(save_path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2663d4b-3431-4bdf-b5cc-84b58c9ee3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da136b7b-b87f-42b2-a7a7-2c4768840270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88933086-71b1-44f6-b311-9aca185e0354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee08d64-7294-4ebd-ba34-6190c2b2dfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c78792-d128-4ec2-b77a-58583cdb2d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50d885-7ef2-44d2-bc68-3a63df120a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29277701-c95e-45c2-a189-1a29151105b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/spiece.model\n",
      "loading file tokenizer.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/aalla4/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "# Running the finetuned question generation model with a sample context\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# the finetuned model name\n",
    "base_model_name = \"google/flan-t5-base\"\n",
    "adapter_path = \"./T5base-flan-lora-adapter-custom-dataset\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "# tokenizer = T5Tokenizer.from_pretrained(base_model_name)\n",
    "# base_model = T5ForConditionalGeneration.from_pretrained(base_model_name)\n",
    "\n",
    "\n",
    "# Load the LoRA config from adapter\n",
    "lora_config = LoraConfig.from_pretrained(adapter_path)\n",
    "\n",
    "# Load base model (T5 base in this case)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(lora_config.base_model_name_or_path)\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_config.base_model_name_or_path)\n",
    "# peft doesn't change the model's tokenizer but why do we do this here, this guarantees consistency â€” the tokenizer used during LoRA fine-tuning and during inference remains the same.\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()\n",
    "\n",
    "def get_question(tag, difficulty, context, answer=\"\", num_questions=3, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate questions using the fine-tuned T5 model.\n",
    "    \n",
    "    Parameters:\n",
    "    - tag: Type of question (e.g., \"short answer\", \"multiple choice question\", \"true or false question\")\n",
    "    - difficulty: \"easy\", \"medium\", \"hard\"\n",
    "    - context: Supporting context or passage\n",
    "    - answer: Optional â€” if you want targeted question generation\n",
    "    - num_questions: Number of diverse questions to generate\n",
    "    - max_length: Max token length of generated output\n",
    "    \n",
    "    Returns:\n",
    "    - List of generated questions as strings\n",
    "    \"\"\"\n",
    "    # Format input text based on whether answer is provided\n",
    "    answer_part = f\"[{answer}]\" if answer else \"\"\n",
    "    input_text = f\"<extra_id_97>{tag} <extra_id_98>{difficulty} <extra_id_99>{answer_part} {context}\"\n",
    "\n",
    "    # Tokenize\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    # Generate questions\n",
    "    output = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_questions,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    # Decode generated questions\n",
    "    for i, out in enumerate(output):\n",
    "        question = tokenizer.decode(out, skip_special_tokens=True)\n",
    "        print(f\"Question {i+1}: {question}\")\n",
    "    \n",
    "    print(\"------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acdd5c41-b494-437d-b485-85615e1aad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving the context and difficulty\n",
    "\n",
    "\n",
    "context = \"Reinforcement Learning (RL) is a dynamic area of machine learning where agents are trained to make a sequence of decisions by interacting with an environment. Each interaction leads to a new state and a scalar reward, which indicates the quality of the action taken. The agentâ€™s objective is to learn an optimal policy that maximizes the total accumulated reward over time. This is different from supervised learning, which requires labeled datasets. In RL, learning is driven by experience and the agent often learns from delayed rewards, making the credit assignment problem a central challenge. The environment is often modeled as a Markov Decision Process (MDP), characterized by states, actions, transition dynamics, and rewards. Algorithms such as Q-learning, SARSA, and Policy Gradient methods are used to find optimal policies. Modern applications employ deep learning to approximate complex functions, giving rise to Deep Reinforcement Learning. Techniques like Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Actor-Critic methods have demonstrated state-of-the-art performance in domains ranging from game playing (e.g., Atari, Go) to robotics and recommendation systems. Exploration-exploitation trade-offs, sample efficiency, and generalization are ongoing challenges in the field. RL has significant potential in real-world decision-making systems.\"\n",
    "\n",
    "\n",
    "difficulty = \"hard\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e4d0f5b-a689-4012-9186-35feabf90502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Reinforcement Learning\n",
      "Question 2: Reinforcement Learning is a dynamic area of machine learning where agents are trained to make a sequence of decisions by interacting with an environment.\n",
      "Question 3: Reinforcement learning\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# short answer question \n",
    "\n",
    "get_question(\n",
    "    tag=\"short answer\",\n",
    "    difficulty=difficulty,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04090c00-2126-4aed-aa4b-78e696a59033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true or false question \n",
    "\n",
    "get_question(\n",
    "    tag=\"true or false question\",\n",
    "    difficulty=difficulty,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172fa7d8-a102-4489-8161-2bd75b797c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple choice question \n",
    "\n",
    "get_question(\n",
    "    tag=\"multiple choice question\",\n",
    "    difficulty=difficulty,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4cd24-50b7-48a5-8027-5fde947698fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879febb-a497-4902-95ad-e62076d69933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
